{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ7ny0lv8Cp3",
        "outputId": "c40f688b-f051-4106-982e-b22040b0b898",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import gym\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "from torch.distributions.relaxed_bernoulli import RelaxedBernoulli\n",
        "from torch.distributions import Bernoulli\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
        "from torch.distributions import Normal\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yRhYGmR8McM"
      },
      "source": [
        "#some tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIFTd9Yl8F3t"
      },
      "source": [
        "def padding_tensor(sequences):\n",
        "    \"\"\"\n",
        "    :param sequences: list of tensors with shape [seq, state dim]\n",
        "    :return: tensor with shape [num, max_seq_length, state dim]\n",
        "    \"\"\"\n",
        "    num = len(sequences)\n",
        "    max_len = max([s.size(0) for s in sequences])\n",
        "    feature_dim = sequences[0].size(-1)\n",
        "    out_dims = (num, max_len, feature_dim)\n",
        "\n",
        "    out_tensor = sequences[0].data.new(*out_dims).fill_(0)\n",
        "\n",
        "    mask = sequences[0].data.new(*out_dims).fill_(0)\n",
        "    for i, tensor in enumerate(sequences):\n",
        "        length = tensor.size(0)\n",
        "        out_tensor[i, :length, :] = tensor\n",
        "        mask[i, :length,:] = 1\n",
        "    return out_tensor, mask\n",
        "\n",
        "\n",
        "def truncate_sequence(list_of_sequence, batch_first, min_len=None):\n",
        "    \"\"\"\n",
        "    list_of_sequence: list of tensor with shape [seq, feature dim]\n",
        "    return : tensor with shape [min_seq_length, batch, fea_dim] or [batch, min_seq_length, fea_dim] if batch_first\n",
        "    \"\"\"\n",
        "    feature_dim = list_of_sequence[0].size(-1)\n",
        "    if min_len is None:\n",
        "        min_len = min([s.size(0) for s in list_of_sequence])\n",
        "\n",
        "    container = torch.zeros(len(list_of_sequence), min_len, feature_dim)\n",
        "    for i in range(len(list_of_sequence)):\n",
        "        #random truncation\n",
        "        #start = np.random.choice((list_of_sequence[i].size(0) - min_len +1))\n",
        "        #container[i] = list_of_sequence[i][start : start+min_len, :]\n",
        "        container[i] = list_of_sequence[i][:min_len,:]\n",
        "    \n",
        "    if batch_first:\n",
        "        return container\n",
        "    else:\n",
        "        return container.permute(1,0,2)\n",
        "\n",
        "def plot_DRNN(true_obs, pred_DRNN):\n",
        "    \n",
        "\n",
        "\n",
        "    position = true_obs[1:,0,0].data.numpy()\n",
        "    velocity = true_obs[1:,0,1].data.numpy()\n",
        "    angle = true_obs[1:,0,2].data.numpy()\n",
        "    angle_v = true_obs[1:,0,3].data.numpy()\n",
        "\n",
        "    #DRNN\n",
        "    pred = torch.cat(pred_DRNN)\n",
        "    position_pred = pred[:,0].data.cpu().numpy()\n",
        "    velocity_pred = pred[:,1].data.cpu().numpy()\n",
        "    angle_pred = pred[:,2].data.cpu().numpy()\n",
        "    angle_velocity_pred = pred[:,3].data.cpu().numpy()\n",
        "\n",
        "\n",
        "\n",
        "    fig, ax = plt.subplots(1,4, figsize = (20,5))\n",
        "    x = np.arange(0, int(position.shape[0]))\n",
        "\n",
        "\n",
        "    ax[0].plot(position, label = 'True')\n",
        "    ax[0].plot(position_pred, label = 'Pred')\n",
        "    ax[0].set_title('Position');\n",
        "    ax[0].legend();\n",
        "\n",
        "    ax[1].plot(velocity, label = 'True')\n",
        "    ax[1].plot(velocity_pred, label = 'Pred')\n",
        "    ax[1].set_title('Velocity')\n",
        "    ax[1].legend();\n",
        "\n",
        "    ax[2].plot(angle, label = 'True')\n",
        "    ax[2].plot(angle_pred, label = 'Pred')\n",
        "    ax[2].set_title('Angle')\n",
        "    ax[2].legend();\n",
        "\n",
        "    ax[3].plot(angle_v, label = 'True')\n",
        "    ax[3].plot(angle_velocity_pred, label = 'Pred')\n",
        "    ax[3].set_title('Angle velocity')\n",
        "    ax[3].legend()\n",
        "\n",
        "def save_model_policy(model, model_optimiser, policy, policy_optimiser, save_model_path, save_policy_path):\n",
        "    save_model_path = save_model_path + \"/model.tar\"\n",
        "    save_policy_path = save_policy_path + \"/policy.tar\" \n",
        "    torch.save({\n",
        "        \"model_dict\": model.state_dict(),\n",
        "        \"trainer_dict\": model_optimiser.state_dict()\n",
        "    }, save_model_path)\n",
        "\n",
        "    torch.save({\n",
        "        \"model_dict\": policy.state_dict(),\n",
        "        \"trainer_dict\": policy_optimiser.state_dict()\n",
        "    }, save_policy_path)\n",
        "\n",
        "\n",
        "\n",
        "    print('Checkpointed')\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYk8LgcE8fsj"
      },
      "source": [
        "#Cart-pole balancing env"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9_o6aK18F6B"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Classic cart-pole system implemented by Rich Sutton et al.\n",
        "Copied from http://incompleteideas.net/sutton/book/code/pole.c\n",
        "permalink: https://perma.cc/C9ZM-652R\n",
        "Modified by Aaditya Ravindran to include friction and random sensor & actuator noise\n",
        "\"\"\"\n",
        "\n",
        "import logging\n",
        "import math\n",
        "import random\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "import numpy as np\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class CartPoleModEnv(gym.Env):\n",
        "    metadata = {\n",
        "            'render.modes': ['human', 'rgb_array'],\n",
        "            'video.frames_per_second' : 50\n",
        "    }\n",
        "\n",
        "    def __init__(self,case):\n",
        "        self.__version__ = \"0.2.0\"\n",
        "        print(\"CartPoleModEnv - Version {}, Noise case: {}\".format(self.__version__,case))\n",
        "        self.gravity = 9.8\n",
        "        self.masscart = 1.0\n",
        "        self.masspole = 0.1\n",
        "        self.total_mass = (self.masspole + self.masscart)\n",
        "        self.length = 0.5 # actually half the pole's length\n",
        "        self.polemass_length = (self.masspole * self.length)\n",
        "        self.seed()\n",
        "\n",
        "        self.origin_case = case\n",
        "        if case<6:          #only model  noise\n",
        "            self.force_mag = 30.0*(1+self.addnoise(case))\n",
        "            self.case = 1\n",
        "        elif case>9:    #both model and data noise\n",
        "            self.force_mag = 30.0*(1+self.addnoise(case))\n",
        "            self.case = 10\n",
        "        else:               #only data noise\n",
        "            self.force_mag = 30.0\n",
        "            self.case = case\n",
        "            \n",
        "        self.tau = 0.02     # seconds between state updates\n",
        "\n",
        "        self.min_action = -1.\n",
        "        self.max_action = 1.0\n",
        "\n",
        "\n",
        "\t\t# Angle at which to fail the episode\n",
        "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
        "        self.x_threshold = 2.4\n",
        "\n",
        "        # Angle limit set to 2 * theta_threshold_radians so failing observation is still within bounds\n",
        "        high = np.array([\n",
        "            self.x_threshold * 2,\n",
        "            np.finfo(np.float32).max,\n",
        "            self.theta_threshold_radians * 2,\n",
        "            np.finfo(np.float32).max])\n",
        "\n",
        "        #self.action_space = spaces.Discrete(2) # AA Set discrete states back to 2\n",
        "        self.action_space = spaces.Box(\n",
        "                low = self.min_action,\n",
        "                high = self.max_action,\n",
        "                shape = (1,) \n",
        "        )\n",
        "\n",
        "        self.observation_space = spaces.Box(-high, high)\n",
        "\n",
        "        self.viewer = None\n",
        "        self.state = None\n",
        "\n",
        "        self.steps_beyond_done = None\n",
        "\n",
        "    def addnoise(self,x):\n",
        "        return {\n",
        "        1 : 0,\n",
        "        2 : self.np_random.uniform(low=-0.05, high=0.05, size=(1,)), #  5% actuator noise ,  small model uniform noise\n",
        "        3 : self.np_random.uniform(low=-0.10, high=0.10, size=(1,)), # 10% actuator noise ,  large model uniform noise\n",
        "        4 : self.np_random.normal(loc=0, scale=np.sqrt(0.10), size=(1,)),                  # small model gaussian noise\n",
        "        5 : self.np_random.normal(loc=0, scale=np.sqrt(0.50), size=(1,)),                 #  large model gaussian noise\n",
        "        6 : self.np_random.uniform(low=-0.05, high=0.05, size=(1,)), #  5% sensor noise ,    small data uniform noise\n",
        "        7 : self.np_random.uniform(low=-0.10, high=0.10, size=(1,)), # 10% sensor noise ,    large data uniform noise\n",
        "        8 : self.np_random.normal(loc=0, scale=np.sqrt(0.10), size=(1,)), # 0.1              small data gaussian noise\n",
        "        9 : self.np_random.normal(loc=0, scale=np.sqrt(0.20), size=(1,)), # 0.2              large data gaussian noise\n",
        "        10: self.np_random.normal(loc = 0, scale = np.sqrt(0.10), size = (1,)),           #  small both gaussian noise\n",
        "        11: self.np_random.normal(loc = 0, scale = np.sqrt(0.50), size = (1,)),          #    large both gaussian noise\n",
        "        }.get(x,1)\n",
        "\n",
        "    def seed(self, seed=None): # Set appropriate seed value\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def stepPhysics(self, force):\n",
        "        x, x_dot, theta, theta_dot = self.state\n",
        "        costheta = math.cos(theta)\n",
        "        sintheta = math.sin(theta)\n",
        "        temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass\n",
        "        thetaacc = (self.gravity * sintheta - costheta * temp) / \\\n",
        "                    (self.length * (4.0/3.0 - self.masspole * costheta * costheta / self.total_mass))\n",
        "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
        "        #noise = self.addnoise(self.case) \n",
        "        x  = (x + self.tau * x_dot)\n",
        "        x_dot = (x_dot + self.tau * xacc)\n",
        "        theta = (theta + self.tau * theta_dot)#*(1 + noise)\n",
        "        theta_dot = (theta_dot + self.tau * thetaacc)\n",
        "        return (x, x_dot, theta, theta_dot)\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        assert self.action_space.contains(action), \"%r (%s) invalid\"%(action, type(action))\n",
        "        force = self.force_mag * float(action)\n",
        "        self.state = self.stepPhysics(force)\n",
        "        x, x_dot, theta, theta_dot = self.state         #true state\n",
        "\n",
        "        #adding measurement noisy to theta\n",
        "        noise = self.addnoise(self.case)\n",
        "        theta = theta * (1+noise)\n",
        "        noise = self.addnoise(self.case)\n",
        "        x = x * (1+noise)\n",
        "        noise = self.addnoise(self.case)\n",
        "        x_dot = x_dot*(1+noise)\n",
        "        noise = self.addnoise(self.case)\n",
        "        theta_dot = theta_dot*(1+noise)\n",
        "\n",
        "        output_state = (x, x_dot, theta, theta_dot) \n",
        "        output_state = np.array(output_state)  \n",
        "\n",
        "\n",
        "        done = x < -self.x_threshold \\\n",
        "            or x > self.x_threshold \\\n",
        "            or theta < -self.theta_threshold_radians \\\n",
        "            or theta > self.theta_threshold_radians\n",
        "        done = bool(done)\n",
        "\n",
        "        if not done:\n",
        "            reward = 1.0\n",
        "        elif self.steps_beyond_done is None:\n",
        "            # Pole just fell!\n",
        "            self.steps_beyond_done = 0\n",
        "            reward = 1.0\n",
        "        else:\n",
        "            if self.steps_beyond_done == 0:\n",
        "                logger.warn(\"You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\")\n",
        "            self.steps_beyond_done += 1\n",
        "            reward = 0.0\n",
        "\n",
        "        #return np.array(self.state), reward, done, {}\n",
        "        return output_state, reward, done, {}\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
        "        self.steps_beyond_done = None\n",
        "\n",
        "        #also reset the force\n",
        "        if self.origin_case<6:          #only model  noise\n",
        "            self.force_mag = 30.0*(1+self.addnoise(self.origin_case))\n",
        "        elif self.origin_case>9:    #both model and data noise\n",
        "            self.force_mag = 30.0*(1+self.addnoise(self.origin_case))\n",
        "        else:               #only data noise\n",
        "            self.force_mag = 30.0\n",
        "\n",
        "        return np.array(self.state)\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        if close:\n",
        "            if self.viewer is not None:\n",
        "                self.viewer.close()\n",
        "                self.viewer = None\n",
        "            return\n",
        "\n",
        "        screen_width = 600\n",
        "        screen_height = 400\n",
        "\n",
        "        world_width = self.x_threshold*2\n",
        "        scale = screen_width/world_width\n",
        "        carty = 100 # TOP OF CART\n",
        "        polewidth = 10.0\n",
        "        polelen = scale * 1.0\n",
        "        cartwidth = 50.0\n",
        "        cartheight = 30.0\n",
        "\n",
        "        if self.viewer is None:\n",
        "            from gym.envs.classic_control import rendering\n",
        "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
        "            l,r,t,b = -cartwidth/2, cartwidth/2, cartheight/2, -cartheight/2\n",
        "            axleoffset =cartheight/4.0\n",
        "            cart = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n",
        "            self.carttrans = rendering.Transform()\n",
        "            cart.add_attr(self.carttrans)\n",
        "            self.viewer.add_geom(cart)\n",
        "            l,r,t,b = -polewidth/2,polewidth/2,polelen-polewidth/2,-polewidth/2\n",
        "            pole = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n",
        "            pole.set_color(.8,.6,.4)\n",
        "            self.poletrans = rendering.Transform(translation=(0, axleoffset))\n",
        "            pole.add_attr(self.poletrans)\n",
        "            pole.add_attr(self.carttrans)\n",
        "            self.viewer.add_geom(pole)\n",
        "            self.axle = rendering.make_circle(polewidth/2)\n",
        "            self.axle.add_attr(self.poletrans)\n",
        "            self.axle.add_attr(self.carttrans)\n",
        "            self.axle.set_color(.5,.5,.8)\n",
        "            self.viewer.add_geom(self.axle)\n",
        "            self.track = rendering.Line((0,carty), (screen_width,carty))\n",
        "            self.track.set_color(0,0,0)\n",
        "            self.viewer.add_geom(self.track)\n",
        "\n",
        "        if self.state is None: return None\n",
        "\n",
        "        x = self.state\n",
        "        cartx = x[0]*scale+screen_width/2.0 # MIDDLE OF CART\n",
        "        self.carttrans.set_translation(cartx, carty)\n",
        "        self.poletrans.set_rotation(-x[2])\n",
        "        return self.viewer.render(return_rgb_array = mode=='rgb_array')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOPv_MIl8jSV"
      },
      "source": [
        "#DRNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3aOHo9b8j-q"
      },
      "source": [
        "class DRNN(nn.Module):\n",
        "    def __init__(self, action_dim, hidden_dim, output_dim, device, mode):\n",
        "        super(DRNN, self).__init__()\n",
        "\n",
        "        self.mode = mode\n",
        "\n",
        "        self.init_encoder = nn.Sequential(\n",
        "            nn.Linear(output_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        if mode == 'RNN':\n",
        "            self.recurrent = nn.RNN(action_dim, hidden_dim)\n",
        "        elif mode == 'LSTM':\n",
        "            self.recurrent = nn.LSTM(action_dim, hidden_dim)\n",
        "        elif mode == 'GRU':\n",
        "            self.recurrent = nn.GRU(action_dim, hidden_dim)\n",
        "        else:\n",
        "            raise ValueError('Mode must be one of RNN, LSTM and GRU')\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim),\n",
        "        )\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, init_x, actions):\n",
        "        \"\"\"\n",
        "        init_x : [batch, output_dim]\n",
        "        actions : [seq, batch, input_dim]\n",
        "        \"\"\"\n",
        "\n",
        "        init_x = init_x.to(self.device)\n",
        "        actions = actions.to(self.device)\n",
        "\n",
        "        init_h = self.init_encoder(init_x).unsqueeze(0).to(self.device)     #[1, batch, hidden]\n",
        "\n",
        "        #print('action dim=', actions.shape, 'init_h dim=',init_h.shape)\n",
        "        if self.mode == 'LSTM':\n",
        "            init_c = torch.zeros_like(init_h).to(self.device)           #zero inital cell state\n",
        "            recurrent_states = self.recurrent(actions, (init_h, init_c))[0]\n",
        "        else:\n",
        "            recurrent_states = self.recurrent(actions, init_h)[0]       #list of rnn hidden state\n",
        "\n",
        "\n",
        "        output_list = []\n",
        "        for h in recurrent_states:\n",
        "            temp_out = self.decoder(h.squeeze(0))       #[batch, output_dim]\n",
        "\n",
        "            #print('temp_out dim', temp_out.shape)\n",
        "            output_list.append(temp_out.unsqueeze(0))\n",
        "        \n",
        "        return output_list\n",
        "\n",
        "    def forward2(self, init_x, actions):\n",
        "        init_x = init_x.to(self.device)\n",
        "        actions = actions.to(self.device)\n",
        "        init_h = self.init_encoder(init_x).unsqueeze(0).to(self.device)\n",
        "        prediction_obs = []\n",
        "\n",
        "        previous_h = init_h\n",
        "        for t in range(actions.size(0)):\n",
        "            previous_a = actions[t]\n",
        "            current_h = self.recurrent(previous_a.unsqueeze(0), previous_h)[-1]\n",
        "\n",
        "            temp_out = self.decoder(current_h.squeeze(0))\n",
        "            prediction_obs.append(temp_out.unsqueeze(0))\n",
        "            previous_h = current_h\n",
        "        return prediction_obs\n",
        "\n",
        "    def imagine(self, init_x, control_f, horizon, plan):\n",
        "        \"\"\"\n",
        "        Given an initial state and the policy function, do model rollout and output sequence of trajectory\n",
        "        init_x : [batch, output]\n",
        "        \"\"\"\n",
        "        init_x = init_x.unsqueeze(0).to(self.device)       #[1,batch,output]\n",
        "\n",
        "        init_h = self.init_encoder(init_x).to(self.device)      #[1, batch, hid]\n",
        "        if self.mode == 'LSTM':\n",
        "            init_c = torch.zeros_like(init_h).to(self.device)\n",
        "            previous_c = init_c\n",
        "        \n",
        "        previous_x = init_x.squeeze(0)         #[batch, output]\n",
        "        previous_h = init_h            #[1, batch, hid]\n",
        "        output_list = []\n",
        "        action_log_prob_list = []\n",
        "        action_list = []\n",
        "        for t in range(horizon):\n",
        "            if plan == 'pg':\n",
        "                action_dist = control_f(previous_x)       #[batch, 1]   \n",
        "                action_samples = action_dist.sample()                      #[batch, 1]\n",
        "            \n",
        "                #compute log prob\n",
        "                action_log_prob = action_dist.log_prob(action_samples)           #[batch, 1]\n",
        "                action_log_prob_list.append(action_log_prob.unsqueeze(0))           #[1, batch, 1]\n",
        "            elif plan == 'rp':\n",
        "                action_samples, _= control_f(previous_x)          #[batch, 1]\n",
        "                action_log_prob_list = 0\n",
        "                action_list.append(action_samples)\n",
        "\n",
        "            if self.mode == 'LSTM':\n",
        "                next_h, next_c = self.recurrent(action_samples.unsqueeze(0), (previous_h, previous_c))[1]   \n",
        "            else:\n",
        "                next_h = self.recurrent(action_samples.unsqueeze(0), previous_h)[1]\n",
        "\n",
        "            next_x = self.decoder(next_h.squeeze(0))            #[batch, output_dim]\n",
        "            output_list.append(next_x.unsqueeze(0))             #[1, batch, output_dim]\n",
        "\n",
        "            previous_h = next_h\n",
        "            previous_x = next_x         #[batch,. output]\n",
        "            if self.mode == 'LSTM':\n",
        "                previous_c = next_c\n",
        "            \n",
        "\n",
        "\n",
        "        output_list = torch.cat(output_list)           #[seq-1, batch, output_dim ]\n",
        "        if plan == 'pg':\n",
        "            action_log_prob_list = torch.cat(action_log_prob_list)  #[seq-1, batch, 1]\n",
        "\n",
        "\n",
        "        return output_list, action_list#action_log_prob_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        #compute cost\n",
        "        cost = 20*cost_f(output_list).detach()                 #[seq-1, batch, 1]   \n",
        "\n",
        "        loss = (cost * action_log_prob_list).sum(0)                 #[batch, 1]\n",
        "        loss = torch.mean(loss)\n",
        "        loss.backward()\n",
        "\n",
        "        \"\"\"\n",
        "    def validate_by_imagination(self, init_x, control_f, plan):\n",
        "\n",
        "        init_x = init_x.unsqueeze(0).to(self.device)       #[1,batch,output]\n",
        "\n",
        "        init_h = self.init_encoder(init_x).to(self.device)      #[1, batch, hid]\n",
        "        if self.mode == 'LSTM':\n",
        "            init_c = torch.zeros_like(init_h).to(self.device)\n",
        "            previous_c = init_c\n",
        "        \n",
        "        previous_x = init_x.squeeze(0)         #[batch, output]\n",
        "        previous_h = init_h            #[1, batch, hid]\n",
        "    \n",
        "        reward = 0\n",
        "        iter = 0\n",
        "        while True:\n",
        "            action_samples, _= control_f(previous_x)          #[batch, 1]\n",
        "            if self.mode == 'LSTM':\n",
        "                next_h, previous_c = self.recurrent(action_samples.unsqueeze(0), (previous_h, previous_c))[1]   \n",
        "            else:\n",
        "                next_h = self.recurrent(action_samples.unsqueeze(0), previous_h)[1]\n",
        "\n",
        "            next_x = self.decoder(next_h.squeeze(0))            #[batch, output_dim]\n",
        "            \n",
        "            reward+=1\n",
        "            iter+=1\n",
        "            \n",
        "            done = next_x[:,0]<-2.4 \\\n",
        "               or next_x[:,0]>2.4 \\\n",
        "               or next_x[:,2]<-12*2*math.pi/360 \\\n",
        "               or next_x[:,2]>12*2*math.pi/360 \\\n",
        "               or iter>=200\n",
        "\n",
        "            done = bool(done)\n",
        "            if done:\n",
        "                break\n",
        "            previous_h = next_h\n",
        "            previous_x = next_x \n",
        "\n",
        "        return reward\n",
        "\n",
        "\n",
        "\n",
        "    def imagine_deterministic(self, init_x, control_f, horizon):\n",
        "        \"\"\"\n",
        "        init_x : [batch ,state_dim]\n",
        "        \"\"\"\n",
        "\n",
        "        init_x = init_x.unsqueeze(0).to(self.device)       #[1,batch,output]\n",
        "\n",
        "        init_h = self.init_encoder(init_x).to(self.device)      #[1, batch, hid]\n",
        "        if self.mode == 'LSTM':\n",
        "            init_c = torch.zeros_like(init_h).to(self.device)\n",
        "            previous_c = init_c\n",
        "        \n",
        "        previous_x = init_x         #[1, batch, output]\n",
        "        previous_h = init_h            #[1, batch, hid]\n",
        "        output_list = []\n",
        "\n",
        "        for t in range(horizon):\n",
        "            \n",
        "            action = control_f(previous_x.squeeze(0))       #[batch, 1]\n",
        "\n",
        "\n",
        "            if self.mode == 'LSTM':\n",
        "                next_h, next_c = self.recurrent(action.unsqueeze(0), (previous_h, previous_c))[1]   \n",
        "            else:\n",
        "                next_h = self.recurrent(action.unsqueeze(0), previous_h)[1]\n",
        "\n",
        "            next_x = self.decoder(next_h.squeeze(0))            #[batch, output_dim]\n",
        "            output_list.append(next_x.unsqueeze(0))             #[1, batch, output_dim]\n",
        "\n",
        "            previous_h = next_h\n",
        "            previous_x = next_x\n",
        "            if self.mode == 'LSTM':\n",
        "                previous_c = next_c\n",
        "\n",
        "        output_list = torch.cat(output_list)           #[seq-1, batch, output_dim ]\n",
        "\n",
        "\n",
        "        return output_list\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZunCg9t08nAz"
      },
      "source": [
        "#Deterministic controller"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz04ACz18kBO"
      },
      "source": [
        "class controller(nn.Module):\n",
        "    def __init__(self, action_dim=1, state_dim=4, deterministic = True,device = 'cuda'):\n",
        "        super(controller, self).__init__()\n",
        "        self.action_dim = action_dim\n",
        "        controller_hid = 16\n",
        "        self.state_dim  = state_dim\n",
        "        init_w = 1e-3\n",
        "\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(self.state_dim,controller_hid),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(controller_hid, controller_hid),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(controller_hid, action_dim)\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #if not deterministic:\n",
        "        #    self.std = 0.1\n",
        "        \n",
        "        #self.deterministic = deterministic\n",
        "        \n",
        "        #nn.Linear(self.state_dim, self.action_dim)        #output a p_logits for action 1\n",
        "        self.device = device\n",
        "        self.optimiser = torch.optim.Adam(self.parameters(), lr = 0.001)\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"\n",
        "        Given states input [batch, state_dim],\n",
        "        \"\"\"\n",
        "        state = state.to(self.device)\n",
        "        #x = F.relu(self.linear1(state))\n",
        "        #x = F.relu(self.linear2(x))\n",
        "        a = self.linear(state)\n",
        "        a =  torch.tanh(a)\n",
        "        log_pi = 0\n",
        "\n",
        "        return a, log_pi\n",
        "\n",
        "\n",
        "        #out_mean = self.linear(state)         #[batch, action_dim]\n",
        "        \n",
        "        #if not self.deterministic:\n",
        "        #    eps = torch.rand_like(out_mean).normal_().to(self.device)           #[batch, action_dim]\n",
        "        #    out = out_mean + self.std * eps\n",
        "        #else:\n",
        "        #    out = out_mean\n",
        "\n",
        "        #if len(out.shape) == 1:\n",
        "        #    out = torch.clamp(out, -1, 1)\n",
        "        #else:\n",
        "        #    out = torch.clamp(out[:,0], -1, 1).unsqueeze(1)             #[1, batch, 1]\n",
        "\n",
        "        #return torch.tanh(out)\n",
        "\n",
        "\n",
        "        #if len(out.shape) == 1:\n",
        "        #    return torch.clamp(out, -1, 1)\n",
        "        #else:\n",
        "        #    clamp_out = torch.clamp(out[:, 0], -1, 1).unsqueeze(-1)\n",
        "        #    return clamp_out\n",
        "    \n",
        "    def make_decision(self, state, behaviour_uncertainty):\n",
        "        \"\"\"\n",
        "        given a state [batch, state_dim], output a action\n",
        "        \"\"\"\n",
        "        state = state.to(self.device)\n",
        "        #x = F.relu(self.linear1(state))\n",
        "        #x = F.relu(self.linear2(x))\n",
        "        a = self.linear(state)\n",
        "        a = torch.tanh(a)\n",
        "\n",
        "        return a.detach()\n",
        "\n",
        "        #out_mean = self.linear(state)\n",
        "        #if not self.deterministic and behaviour_uncertainty:\n",
        "        #    eps = torch.rand_like(out_mean).normal_().to(self.device) \n",
        "        #    out = out_mean + self.std * eps\n",
        "        #else:\n",
        "        #    out = out_mean\n",
        "        #if len(out.shape) == 1:\n",
        "\n",
        "        #    out = torch.clamp(out, -1, 1)\n",
        "        #else:\n",
        "        #    out = torch.clamp(out[:,0], -1, 1)\n",
        "        #return out.detach()\n",
        "        #return torch.tanh(out)\n",
        "    def pg_train(self, num_epoch, initial_state, horizon, cost_f, model_imagine_f, w_uncertainty, e_uncertainty,gamma = 0.95):\n",
        "        \"\"\"\n",
        "        initial_state : [batch, state_dim]\n",
        "\n",
        "        \"\"\"\n",
        "        loss_list = []\n",
        "        num_particle = 100\n",
        "        initial_state = initial_state.expand(num_particle, self.state_dim)\n",
        "\n",
        "        for e in range(num_epoch):\n",
        "            output_matrix, action_log_prob_matrix = model_imagine_f(initial_state, self.forward, horizon, plan = 'pg',\n",
        "                                                                    W_uncertainty = w_uncertainty, e_uncertainty = e_uncertainty)\n",
        "            cost = cost_f(output_matrix).detach()               #[seq-1, batch, 1]  \n",
        "            \n",
        "            cost = cost * torch.tensor([gamma**(t+1) for t in range(cost.size(0))]).unsqueeze(-1).unsqueeze(-1).to(self.device)\n",
        "\n",
        "            #baseline = torch.mean(cost, dim = 0).unsqueeze(0)       #[1, batch, 1]\n",
        "            #cost = cost - torch.mean(cost, dim = 0).unsqueeze(0)\n",
        "            #loss = ((cost-baseline) * action_log_prob_matrix).sum(0)\n",
        "            loss = cost.sum(0) * action_log_prob_matrix.sum(0) \n",
        "\n",
        "            loss = loss.sum()\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(self.parameters(), 5)\n",
        "\n",
        "            self.optimiser.step()\n",
        "            loss_list.append(loss.item())\n",
        "            if e%50 == 0:\n",
        "                print('Epoch = {}; Policy gradient training loss = {}'.format(e, loss.item()))\n",
        "        return loss_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    def rp_train(self, num_epoch, num_particle,initial_state, horizon , cost_f, model_imagine_f, gamma = 0.9):\n",
        "        \"\"\"\n",
        "        From an initial state, use mode imagination function to make prediction of the next state accordin to the action proposed by\n",
        "        the controller, we fixed the horizon and compute the total reward of the trajectory, from which the gradient w.r.t policy \n",
        "        parameters is taken.\n",
        "        inital_state: [batch, output_dim]\n",
        "        \"\"\"\n",
        "        loss_list = []\n",
        "        num_particle = num_particle\n",
        "        initial_state = initial_state.expand(num_particle, self.state_dim)\n",
        "\n",
        "        cost_mean_list = []\n",
        "        cost_std_list = []\n",
        "        \n",
        "        for e in range(num_epoch):\n",
        "            self.optimiser.zero_grad()\n",
        "            output_matrix, action_matrix= model_imagine_f(initial_state, self.forward, horizon, plan = 'rp') \n",
        "            \n",
        "            self.action_matrix = action_matrix\n",
        "            self.temp_output_matrix = torch.cat([initial_state.unsqueeze(0).to(self.device), output_matrix], dim = 0)\n",
        "\n",
        "            cost = cost_f(output_matrix)                 #[seq-1, batch, 1]  \n",
        "\n",
        "            mean_cost = cost.data.sum(0).mean(0)      #[]\n",
        "            std_cost = cost.data.sum(0).std(0)\n",
        "            cost_mean_list.append(mean_cost)\n",
        "            cost_std_list.append(std_cost)\n",
        "\n",
        "            #multiply by discount factor\n",
        "            #cost = cost *  ((torch.arange(cost.size(0)+1,1,-1).float()).unsqueeze(-1).unsqueeze(-1)/cost.size(0)\n",
        "            #                    ).expand(cost.shape).float().to(self.device)\n",
        "            \n",
        "            cost = cost * torch.tensor([gamma**(t+1) for t in range(cost.size(0))]).unsqueeze(-1).unsqueeze(-1).to(self.device)\n",
        "\n",
        "            loss = cost.sum()      #[batch, 1]\n",
        "            #loss = torch.exp(action_log_prob_matrix.sum(0)) * cost.sum(0)\n",
        "            #loss = (cost * action_log_prob_matrix).sum(0)                 #[batch, 1]\n",
        "            loss.backward()\n",
        "\n",
        "            nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "            self.optimiser.step()\n",
        "            loss_list.append(loss.item())\n",
        "            #print('policy loss = {}', loss.item())\n",
        "            #print('Epoch = {}; Policy gradient training loss = {}; Cost: mean {} std {}.'.format(e, loss.item()/num_particle,\n",
        "            #                                                                                        mean_cost.item(), std_cost.item()))\n",
        "\n",
        "\n",
        "        return loss_list, torch.cat(cost_mean_list), torch.cat(cost_std_list)\n",
        "\n",
        "    def rp_validate(self, num_particle, initial_state, horizon, cost_f, model_imagine_f, gamma = 1, print_trajectory = False):\n",
        "        initial_state = initial_state.expand(num_particle, self.state_dim)\n",
        "        output_matrix, action_matrix=  model_imagine_f(initial_state, self.forward, horizon, plan = 'rp')\n",
        "        cost = cost_f(output_matrix)\n",
        "\n",
        "        mean_cost = cost.data.sum(0).mean(0)\n",
        "\n",
        "        #outputing trajectory\n",
        "        if print_trajectory:\n",
        "            mean_output = output_matrix.mean(1)     #[seq-1, output]\n",
        "            std_output = output_matrix.std(1)\n",
        "            return mean_output, std_output, action_matrix\n",
        "\n",
        "\n",
        "        return mean_cost.item()\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6FxogyI8slB"
      },
      "source": [
        "#Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxMzGRSZ8uX1"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, env_case,state_dim = 4, action_dim = 1, device='cuda', rand_seed = 1):\n",
        "        \n",
        "        self.env =  CartPoleModEnv(case = env_case)\n",
        "        self.env_case = env_case\n",
        "\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.device = device\n",
        "        self.observations_list = []\n",
        "        self.actions_list= []\n",
        "        self.MSEloss = nn.MSELoss()\n",
        "\n",
        "        self.model = DRNN(action_dim, 32, state_dim, device, 'LSTM').to(device)\n",
        "        self.model_optimiser = torch.optim.Adam(self.model.parameters(), lr = 0.001)\n",
        "        self.mseloss = nn.MSELoss()\n",
        "        self.model_training_loss_list = []\n",
        "\n",
        "        self.policy = controller(action_dim, state_dim,device).to(device)\n",
        "        #np.random.seed(rand_seed)\n",
        "        #torch.manual_seed(rand_seed)\n",
        "        #self.env.seed(rand_seed)\n",
        "\n",
        "    \n",
        "    def env_rollout(self, if_remember, plan, behaviour_uncertainty):\n",
        "        \"\"\"\n",
        "        interaction with the environment using the current policy. \n",
        "        \"\"\"\n",
        "        done = False\n",
        "        state = self.env.reset()\n",
        "        total_reward = 0\n",
        "        i = 0\n",
        "        temp_obs_list = []\n",
        "        temp_actions_list = []\n",
        "\n",
        "        if if_remember:\n",
        "            temp_obs_list.append(torch.tensor(state))\n",
        "        \n",
        "        while not done:\n",
        "            i+=1\n",
        "            \n",
        "            if plan == 'random':\n",
        "                action = self.env.action_space.sample()\n",
        "            elif plan == 'pg' or 'rp':\n",
        "                state_tensor = torch.tensor(np.vstack(state)).float().squeeze()\n",
        "\n",
        "                action = self.policy.make_decision(state_tensor.to(self.device), behaviour_uncertainty)\n",
        "                action = action.detach().cpu().numpy()\n",
        "                \n",
        "            else:\n",
        "                raise NotImplementedError\n",
        "            #print('action = ', action)\n",
        "\n",
        "            next_state, reward, _, _  = self.env.step(action)\n",
        "\n",
        "            done = next_state[0] < -2.4 \\\n",
        "                or next_state[0] > 2.4 \\\n",
        "                or next_state[2] < -12 * 2 * math.pi / 360 \\\n",
        "                or next_state[2] > 12 * 2 * math.pi / 360 \\\n",
        "                or i >= 200\n",
        "\n",
        "\n",
        "            if if_remember:\n",
        "\n",
        "                temp_obs_list.append(torch.tensor(np.vstack(next_state)).squeeze())\n",
        "\n",
        "                temp_actions_list.append(torch.tensor(action).float())\n",
        "\n",
        "            \n",
        "            state = next_state\n",
        "            total_reward += 1\n",
        "\n",
        "        if if_remember:\n",
        "            self.observations_list.append(torch.stack(temp_obs_list).float())       #list of shape [seq, output]\n",
        "            self.actions_list.append(torch.stack(temp_actions_list).float())        #list of shape [seq-1, 1]\n",
        "\n",
        "        return total_reward\n",
        "    \n",
        "    def model_learning(self, num_epoch, num_batch):\n",
        "        \"\"\"\n",
        "        perform model leanring using data self.observation_list and self.actions_list; since the data has variable length, one could \n",
        "        try truncate the data into same length or pack_padded_sequence, but here we would simply train each single sample in a batch,\n",
        "        and during each epoch, the parameter is only updated once using part of the dataset\n",
        "        num_epoch : number of training epoch\n",
        "        num_batch: this is actually number of samples we want the model to be trained on during each epoch\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "        for e in range(num_epoch):\n",
        "            self.model_optimiser.zero_grad()\n",
        "\n",
        "            idx = np.random.choice(len(self.observations_list), num_batch)\n",
        "            trun_obs = truncate_sequence([self.observations_list[i] for i in idx], batch_first = False)\n",
        "            trun_actions = truncate_sequence([self.actions_list[j] for j in idx], batch_first = False)\n",
        "\n",
        "            pred = self.model(trun_obs[0,:,:], trun_actions)\n",
        "            loss = self.MSEloss(torch.cat(pred), trun_obs[1:,:,:].to('cuda'))\n",
        "            loss.backward()\n",
        "\n",
        "            \n",
        "\n",
        "            #for i in idx:\n",
        "            #    training_obs = self.observations_list[i].unsqueeze(1)       #[seq, 1, output]\n",
        "            #    training_actions = self.actions_list[i]                      #[seq-1, 1]\n",
        "\n",
        "            #    pred = self.model(training_obs[0,:,:], training_actions.unsqueeze(-1))\n",
        "            #    loss = self.mseloss(torch.cat(pred).unsqueeze(1), training_obs[1:, :, :].to(self.device))\n",
        "            #    temp_loss += loss\n",
        "            #temp_loss.backward()\n",
        "\n",
        "            self.model_optimiser.step()\n",
        "            self.model_training_loss_list.append(loss.item())\n",
        "\n",
        "            if e%500 == 0:\n",
        "                print('Epoch:{}; loss = {}.'.format(e, loss.item()))\n",
        "\n",
        "\n",
        "    def cost(self, state):\n",
        "        \"\"\"\n",
        "        cost = 5*angle^2 + position^2\n",
        "        state : [seq, batch, output]\n",
        "        return [seq, batch, 1]\n",
        "        \"\"\"\n",
        "        return (5*state[:,:,2]**2 + state[:,:,0]**2).unsqueeze(-1)      #[seq, batch, 1]\n",
        "\n",
        "    '''\n",
        "    def cost(self, states, sigma=0.25):\n",
        "        \"\"\"\n",
        "        states : [seq, batch, output]\n",
        "        return : [seq, batch, 1]\n",
        "        \"\"\"\n",
        "        l = 0.6\n",
        "        seq_length = states.size(0)\n",
        "        batch_size = states.size(1)\n",
        "        feature_dim = states.size(-1)\n",
        "        \n",
        "        goal = Variable(torch.FloatTensor([0.0, l])).unsqueeze(0).unsqueeze(0).expand(seq_length,1, 2).to(self.device)     #[seq, 1,2]\n",
        "\n",
        "        # Cart position\n",
        "        cart_x = states[:,:, 0]         #[seq, batch]\n",
        "        # Pole angle\n",
        "        thetas = states[:,:,2]          #[seq, bnatch]\n",
        "        # Pole position\n",
        "        x = torch.sin(thetas)*l         #[seq, batch]\n",
        "        y = torch.cos(thetas)*l\n",
        "        positions = torch.stack([cart_x + x, y], -1)             #[seq, batch, 2]\n",
        "\n",
        "        \n",
        "        squared_distance = torch.sum((goal - positions)**2, -1).unsqueeze(-1)          #[]\n",
        "\n",
        "        squared_sigma = sigma**2\n",
        "        cost = 1 - torch.exp(-0.5*squared_distance/squared_sigma)\n",
        "        \n",
        "        return cost\n",
        "    '''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def policy_learning(self, imagine_num, num_particle, num_epoch, batch_size, horizon, plan, plot = False):\n",
        "        \"\"\"\n",
        "        we utilise the current learned model to do policy learning on imagined data\n",
        "        num_epoch : number of epochs we want to run our policy gradient for\n",
        "        batch_size : number of samples we want to train the policy on/ number of initial states\n",
        "\n",
        "        we creat batch_size number of initial state, the model then rollout for a fixed length(horizon), the sum of cost for each \n",
        "        imagined trajectory is computed, from which the gradient is taken w.r.t the policy parameters\n",
        "        \"\"\"\n",
        "        #creat inital states \n",
        "        for i in range(imagine_num):\n",
        "            #initial_state = []\n",
        "            #for b in range(batch_size):\n",
        "            #    init_x = self.env.reset()\n",
        "            #    initial_state.append(torch.tensor(init_x).float())\n",
        "            #initial_state = torch.stack(initial_state)          #[batch, output]\n",
        "            initial_state = torch.tensor(self.env.reset()).unsqueeze(0).float()         #[1, output]\n",
        "            if plot:\n",
        "                initial_state = torch.zeros_like(initial_state)\n",
        "\n",
        "            #learn the policy parameter using current model\n",
        "\n",
        "            model_f = self.model.imagine\n",
        "\n",
        "            if plan == 'pg':\n",
        "                policy_train_loss = self.policy.pg_train(num_epoch, num_particle,initial_state, horizon , self.cost, model_f, gamma=1)\n",
        "            elif plan == 'rp':\n",
        "                policy_train_loss = self.policy.rp_train(num_epoch, num_particle, initial_state, horizon, self.cost, model_f, gamma = 0.95)\n",
        "        \"\"\"\n",
        "        total_reward=[]\n",
        "        for i in range(20):\n",
        "            init_x = torch.tensor(self.env.reset()).unsqueeze(0).float()\n",
        "            imagine_reward = self.model.validate_by_imagination(init_x, self.policy.forward, plan)\n",
        "            total_reward.append(imagine_reward)\n",
        "        mean_reward = np.mean(total_reward)\n",
        "        std_reward = np.std(total_reward)\n",
        "        print('Training reward: mean {}, std {}'.format(mean_reward, std_reward))\n",
        "\n",
        "        return mean_reward, std_reward\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        total_cost10 = []\n",
        "        for i in range(20):\n",
        "            initial_state = torch.tensor(self.env.reset()).unsqueeze(0).float()\n",
        "            mean_cost = self.policy.rp_validate(num_particle, initial_state, 10 ,self.cost, model_f, gamma=1)\n",
        "            total_cost10.append(mean_cost)\n",
        "        mean_cost10 = np.mean(total_cost10)\n",
        "        std_cost10 = np.mean(total_cost10)\n",
        "\n",
        "        total_cost100 = []\n",
        "        for i in range(20):\n",
        "            initial_state = torch.tensor(self.env.reset()).unsqueeze(0).float()\n",
        "            mean_cost = self.policy.rp_validate(num_particle, initial_state, 100 ,self.cost, model_f, gamma=1)\n",
        "            total_cost100.append(mean_cost)\n",
        "        mean_cost100 = np.mean(total_cost100)\n",
        "        std_cost100 = np.mean(total_cost100)\n",
        "\n",
        "        return mean_cost10, std_cost10, mean_cost100, std_cost100\n",
        "        \"\"\"\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xjTBeMC8yFH"
      },
      "source": [
        "#Learning-Planning iterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aAH7uA_815n",
        "outputId": "b51afc34-63d5-4744-ab16-063b8763a860",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 940
        }
      },
      "source": [
        "time1 = time.time()\n",
        "testing_reward_list = []\n",
        "mean_training_reward_list = []\n",
        "std_training_reward_list = []\n",
        "device = 'cuda'\n",
        "\n",
        "behaviour_uncertainty = False\n",
        "deterministic = False\n",
        "plan = 'rp'\n",
        "num_data = 10\n",
        "\n",
        "torch.manual_seed(1)\n",
        "agent = Agent(env_case = 1, device = device)\n",
        "for i in range(num_data):\n",
        "     _ = agent.env_rollout(True, behaviour_uncertainty = behaviour_uncertainty,plan = 'random')\n",
        "\n",
        "agent.model_learning(num_epoch=1000, num_batch = 10)\n",
        "#mean_reward, std_reward = \n",
        "agent.policy_learning(imagine_num=50, num_particle = 1000, num_epoch = 1, batch_size = 10, horizon = 10, plan = plan)\n",
        "#mean_training_reward_list.append(mean_reward)\n",
        "#std_training_reward_list.append(std_reward)\n",
        "\n",
        "print(\"--------------------Testing------------------\")\n",
        "avg_rewards = 0\n",
        "for j in range(20):\n",
        "    rewards = agent.env_rollout(if_remember=False, behaviour_uncertainty = behaviour_uncertainty,plan = plan)\n",
        "    print(j, rewards)\n",
        "    avg_rewards += rewards\n",
        "avg_rewards = avg_rewards/20\n",
        "testing_reward_list.append(avg_rewards)\n",
        "print('Total trajs:', j, avg_rewards)\n",
        "if avg_rewards > 200:\n",
        "    print('success')\n",
        "\n",
        "\n",
        "\n",
        "mean_cost10_list = []\n",
        "std_cost10_list = []\n",
        "mean_cost100_list = []\n",
        "std_cost100_list = []\n",
        "\n",
        "avg_data_length_list = []\n",
        "\n",
        "imagine_pred_list = []\n",
        "imagine_action_list = []\n",
        "\n",
        "for i in range(100):\n",
        "    print('Epoch = ',i+1)\n",
        "    _ = agent.env_rollout(True, behaviour_uncertainty = behaviour_uncertainty,plan = plan)\n",
        "    total = 0\n",
        "    for i in range(len(agent.observations_list)):\n",
        "        total+=len(agent.observations_list[i])\n",
        "    print('Average training data length = ',total/len(agent.observations_list))\n",
        "    avg_data_length_list.append(total/len(agent.observations_list))\n",
        "    \n",
        "    agent.model_learning(num_epoch=1000, num_batch = 10)\n",
        "\n",
        "    #mean_reward, std_reward = \n",
        "    agent.policy_learning(imagine_num=50, num_particle = 1000, num_epoch = 1, batch_size = 10, horizon = 10, plan = plan)\n",
        "    #mean_training_reward_list.append(mean_reward)\n",
        "    #std_training_reward_list.append(std_reward)\n",
        "    \"\"\"\n",
        "    if i%10==0:\n",
        "        initial_state = torch.tensor(agent.env.reset()).unsqueeze(0).float()\n",
        "        mean_pred, std_pred, action_matrix= agent.policy.rp_validate(num_particle=1000, initial_state = initial_state, \n",
        "                                                horizon = 100 ,cost_f = agent.cost, model_imagine_f=\n",
        "                                                agent.model.imagine, gamma=1, print_trajectory = True)\n",
        "        imagine_pred_list.append(mean_pred)\n",
        "        imagine_action_list.append(torch.stack(action_matrix)[:,0,:])\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    total_cost10 = []\n",
        "    for i in range(20):\n",
        "        initial_state = torch.tensor(agent.env.reset()).unsqueeze(0).float()\n",
        "        mean_cost = agent.policy.rp_validate(num_particle=1000, initial_state = initial_state, \n",
        "                                             horizon = 10 ,cost_f = agent.cost, model_imagine_f=\n",
        "                                             agent.model.imagine, gamma=1)\n",
        "        total_cost10.append(mean_cost)\n",
        "    mean_cost10 = np.mean(total_cost10)\n",
        "    std_cost10 = np.mean(total_cost10)\n",
        "    mean_cost10_list.append(mean_cost10)\n",
        "    std_cost10_list.append(std_cost10)\n",
        "\n",
        "    total_cost100 = []\n",
        "    for i in range(20):\n",
        "        initial_state = torch.tensor(agent.env.reset()).unsqueeze(0).float()\n",
        "        mean_cost = agent.policy.rp_validate(num_particle=1000, initial_state = initial_state, \n",
        "                                             horizon = 100 ,cost_f = agent.cost, model_imagine_f=\n",
        "                                             agent.model.imagine, gamma=1)        \n",
        "        total_cost100.append(mean_cost)\n",
        "    mean_cost100 = np.mean(total_cost100)\n",
        "    std_cost100 = np.mean(total_cost100)\n",
        "    mean_cost100_list.append(mean_cost100)\n",
        "    std_cost100_list.append(std_cost100)\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"--------------------Testing------------------\")\n",
        "    avg_rewards = 0\n",
        "    for j in range(20):\n",
        "        rewards = agent.env_rollout(if_remember=False, behaviour_uncertainty = behaviour_uncertainty,plan = plan)\n",
        "        print(j, rewards)\n",
        "        avg_rewards += rewards\n",
        "    avg_rewards = avg_rewards/20\n",
        "    testing_reward_list.append(avg_rewards)\n",
        "    print('Total trajs:', j, avg_rewards)\n",
        "    if avg_rewards > 200:\n",
        "        print('success')\n",
        "\n",
        "\n",
        "time2 = time.time()\n",
        "print(time2-time1)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CartPoleModEnv - Version 0.2.0, Noise case: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:0; loss = 0.4223226308822632.\n",
            "Epoch:500; loss = 0.0034770409110933542.\n",
            "--------------------Testing------------------\n",
            "0 20\n",
            "1 17\n",
            "2 24\n",
            "3 16\n",
            "4 19\n",
            "5 19\n",
            "6 16\n",
            "7 15\n",
            "8 19\n",
            "9 16\n",
            "10 15\n",
            "11 19\n",
            "12 21\n",
            "13 20\n",
            "14 22\n",
            "15 16\n",
            "16 16\n",
            "17 21\n",
            "18 19\n",
            "19 17\n",
            "Total trajs: 19 18.35\n",
            "Epoch =  1\n",
            "Average training data length =  16.181818181818183\n",
            "Epoch:0; loss = 0.0007143482798710465.\n",
            "Epoch:500; loss = 0.0007419626927003264.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-708b8cb4a917>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mavg_data_length_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m#mean_reward, std_reward =\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-245410a477a7>\u001b[0m in \u001b[0;36mmodel_learning\u001b[0;34m(self, num_epoch, num_batch)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrun_obs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrun_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSEloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrun_obs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDWbESrO8kDf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
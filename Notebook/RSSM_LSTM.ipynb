{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RSSM-LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFyLLhjh9u4h"
      },
      "source": [
        "import gym\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.distributions.relaxed_bernoulli import RelaxedBernoulli\n",
        "from torch.distributions import Bernoulli\n",
        "from torch.distributions.categorical import Categorical\n",
        "from torch.distributions.kl import kl_divergence\n",
        "from torch.distributions import Normal\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECMYAtIb92d6"
      },
      "source": [
        "#Some tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic4wSX0893Xw"
      },
      "source": [
        "def padding_tensor(sequences):\n",
        "    \"\"\"\n",
        "    :param sequences: list of tensors with shape [seq, state dim]\n",
        "    :return: tensor with shape [num, max_seq_length, state dim]\n",
        "    \"\"\"\n",
        "    num = len(sequences)\n",
        "    max_len = max([s.size(0) for s in sequences])\n",
        "    feature_dim = sequences[0].size(-1)\n",
        "    out_dims = (num, max_len, feature_dim)\n",
        "\n",
        "    out_tensor = sequences[0].data.new(*out_dims).fill_(0)\n",
        "\n",
        "    mask = sequences[0].data.new(*out_dims).fill_(0)\n",
        "    for i, tensor in enumerate(sequences):\n",
        "        length = tensor.size(0)\n",
        "        out_tensor[i, :length, :] = tensorx\n",
        "        mask[i, :length,:] = 1\n",
        "    return out_tensor, mask\n",
        "\n",
        "\n",
        "def truncate_sequence(list_of_sequence, batch_first, min_len=None):\n",
        "    \"\"\"\n",
        "    list_of_sequence: list of tensor with shape [seq, feature dim]\n",
        "    return : tensor with shape [min_seq_length, batch, fea_dim] or [batch, min_seq_length, fea_dim] if batch_first\n",
        "    \"\"\"\n",
        "    feature_dim = list_of_sequence[0].size(-1)\n",
        "    if min_len is None:\n",
        "        min_len = min([s.size(0) for s in list_of_sequence])\n",
        "\n",
        "\n",
        "    container = torch.zeros(len(list_of_sequence), min_len, feature_dim)\n",
        "    for i in range(len(list_of_sequence)):\n",
        "        #random truncation\n",
        "        #start = np.random.choice((list_of_sequence[i].size(0) - min_len +1))\n",
        "        #container[i] = list_of_sequence[i][start : start+min_len, :]\n",
        "        container[i] = list_of_sequence[i][:min_len,:]\n",
        "    \n",
        "    if batch_first:\n",
        "        return container\n",
        "    else:\n",
        "        return container.permute(1,0,2)\n",
        "\n",
        "\n",
        "\n",
        "def save_model_policy(model, model_optimiser, policy, policy_optimiser, save_model_path, save_policy_path):\n",
        "    save_model_path = save_model_path + \"/model.tar\"\n",
        "    save_policy_path = save_policy_path + \"/policy.tar\" \n",
        "    torch.save({\n",
        "        \"model_dict\": model.state_dict(),\n",
        "        \"trainer_dict\": model_optimiser.state_dict()\n",
        "    }, save_model_path)\n",
        "\n",
        "    torch.save({\n",
        "        \"model_dict\": policy.state_dict(),\n",
        "        \"trainer_dict\": policy_optimiser.state_dict()\n",
        "    }, save_policy_path)\n",
        "\n",
        "\n",
        "\n",
        "    print('Checkpointed')\n",
        "\n",
        "def plot_LLB(true_obs, mean, std):\n",
        "\n",
        "    position = true_obs[1:,0,0].data.numpy()\n",
        "    velocity = true_obs[1:,0,1].data.numpy()\n",
        "    angle = true_obs[1:,0,2].data.numpy()\n",
        "    angle_v = true_obs[1:,0,3].data.numpy()\n",
        "\n",
        "    lower = mean - std      #[seq-1, output_dim]\n",
        "    upper = mean + std\n",
        "    position_mean = mean[:,0].data.cpu().numpy()\n",
        "    velocity_mean = mean[:,1].data.cpu().numpy()\n",
        "    angle_mean = mean[:,2].data.cpu().numpy()\n",
        "    angle_v_mean = mean[:,3].data.cpu().numpy()\n",
        "\n",
        "    lower_position = lower[:,0].data.cpu().numpy()\n",
        "    lower_velocity = lower[:,1].data.cpu().numpy()\n",
        "    lower_angle = lower[:,2].data.cpu().numpy()\n",
        "    lower_angle_v = lower[:,3].data.cpu().numpy()\n",
        "\n",
        "\n",
        "    upper_position = upper[:,0].data.cpu().numpy()\n",
        "    upper_velocity = upper[:,1].data.cpu().numpy()\n",
        "    upper_angle = upper[:,2].data.cpu().numpy()\n",
        "    upper_angle_v = upper[:,3].data.cpu().numpy()\n",
        "\n",
        "    fig, ax = plt.subplots(1,4, figsize = (20,5))\n",
        "    x = np.arange(0, position.shape[0])\n",
        "\n",
        "    ax[0].plot(position, label = 'True')\n",
        "    ax[0].plot(x, position_mean, label = 'mean')\n",
        "    ax[0].fill_between(x, upper_position, lower_position, facecolor='grey',\n",
        "                                    color = 'grey', alpha = 0.2)\n",
        "    ax[0].set_title('Position');\n",
        "    ax[0].legend();\n",
        "\n",
        "    ax[1].plot(velocity, label = 'True')\n",
        "    ax[1].plot(x, velocity_mean, label = 'mean')\n",
        "    ax[1].fill_between(x, upper_velocity, lower_velocity, facecolor='grey',\n",
        "                                    color = 'grey', alpha = 0.2)\n",
        "    ax[1].set_title('Velocity')\n",
        "    ax[1].legend();\n",
        "\n",
        "\n",
        "    ax[2].plot(angle, label = 'True')\n",
        "    ax[2].plot(x, angle_mean, label = 'mean')\n",
        "    ax[2].fill_between(x, upper_angle, lower_angle, facecolor='grey',\n",
        "                                    color = 'grey', alpha = 0.2)\n",
        "    ax[2].set_title('angle')\n",
        "    ax[2].legend();\n",
        "\n",
        "    ax[3].plot(angle_v, label = 'True')\n",
        "    ax[3].plot(x, angle_v_mean, label = 'mean')\n",
        "    ax[3].fill_between(x, upper_angle_v, lower_angle_v, facecolor='grey',\n",
        "                                    color = 'grey', alpha = 0.2)\n",
        "    ax[3].set_title('angle velocity')\n",
        "    ax[3].legend();\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYqER0jc94Kc"
      },
      "source": [
        "#Env"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03if2UTZ97Mg"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Classic cart-pole system implemented by Rich Sutton et al.\n",
        "Copied from http://incompleteideas.net/sutton/book/code/pole.c\n",
        "permalink: https://perma.cc/C9ZM-652R\n",
        "Modified by Aaditya Ravindran to include friction and random sensor & actuator noise\n",
        "\"\"\"\n",
        "\n",
        "import logging\n",
        "import math\n",
        "import random\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "import numpy as np\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class CartPoleModEnv(gym.Env):\n",
        "    metadata = {\n",
        "            'render.modes': ['human', 'rgb_array'],\n",
        "            'video.frames_per_second' : 50\n",
        "    }\n",
        "\n",
        "    def __init__(self,case):\n",
        "        self.__version__ = \"0.2.0\"\n",
        "        print(\"CartPoleModEnv - Version {}, Noise case: {}\".format(self.__version__,case))\n",
        "        self.gravity = 9.8\n",
        "        self.masscart = 1.0\n",
        "        self.masspole = 0.1\n",
        "        self.total_mass = (self.masspole + self.masscart)\n",
        "        self.length = 0.5 # actually half the pole's length\n",
        "        self.polemass_length = (self.masspole * self.length)\n",
        "        self.seed()\n",
        "\n",
        "        self.origin_case = case\n",
        "        if case<6:          #only model  noise\n",
        "            self.force_mag = 30.0*(1+self.addnoise(case))\n",
        "            self.case = 1\n",
        "        elif case>9:    #both model and data noise\n",
        "            self.force_mag = 30.0*(1+self.addnoise(case))\n",
        "            self.case = 10\n",
        "        else:               #only data noise\n",
        "            self.force_mag = 30.0\n",
        "            self.case = case\n",
        "            \n",
        "        self.tau = 0.02     # seconds between state updates\n",
        "\n",
        "        self.min_action = -1.\n",
        "        self.max_action = 1.0\n",
        "\n",
        "\n",
        "\t\t# Angle at which to fail the episode\n",
        "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
        "        self.x_threshold = 2.4\n",
        "\n",
        "        # Angle limit set to 2 * theta_threshold_radians so failing observation is still within bounds\n",
        "        high = np.array([\n",
        "            self.x_threshold * 2,\n",
        "            np.finfo(np.float32).max,\n",
        "            self.theta_threshold_radians * 2,\n",
        "            np.finfo(np.float32).max])\n",
        "\n",
        "        #self.action_space = spaces.Discrete(2) # AA Set discrete states back to 2\n",
        "        self.action_space = spaces.Box(\n",
        "                low = self.min_action,\n",
        "                high = self.max_action,\n",
        "                shape = (1,) \n",
        "        )\n",
        "\n",
        "        self.observation_space = spaces.Box(-high, high)\n",
        "\n",
        "        self.viewer = None\n",
        "        self.state = None\n",
        "\n",
        "        self.steps_beyond_done = None\n",
        "\n",
        "    def addnoise(self,x):\n",
        "        return {\n",
        "        1 : 0,\n",
        "        2 : self.np_random.uniform(low=-0.05, high=0.05, size=(1,)), #  5% actuator noise ,  small model uniform noise\n",
        "        3 : self.np_random.uniform(low=-0.10, high=0.10, size=(1,)), # 10% actuator noise ,  large model uniform noise\n",
        "        4 : self.np_random.normal(loc=0, scale=np.sqrt(0.10), size=(1,)),                  # small model gaussian noise\n",
        "        5 : self.np_random.normal(loc=0, scale=np.sqrt(0.50), size=(1,)),                 #  large model gaussian noise\n",
        "        6 : self.np_random.uniform(low=-0.05, high=0.05, size=(1,)), #  5% sensor noise ,    small data uniform noise\n",
        "        7 : self.np_random.uniform(low=-0.10, high=0.10, size=(1,)), # 10% sensor noise ,    large data uniform noise\n",
        "        8 : self.np_random.normal(loc=0, scale=np.sqrt(0.10), size=(1,)), # 0.1              small data gaussian noise\n",
        "        9 : self.np_random.normal(loc=0, scale=np.sqrt(0.20), size=(1,)), # 0.2              large data gaussian noise\n",
        "        10: self.np_random.normal(loc = 0, scale = np.sqrt(0.10), size = (1,)),           #  small both gaussian noise\n",
        "        11: self.np_random.normal(loc = 0, scale = np.sqrt(0.50), size = (1,)),          #    large both gaussian noise\n",
        "        }.get(x,1)\n",
        "\n",
        "    def seed(self, seed=None): # Set appropriate seed value\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def stepPhysics(self, force):\n",
        "        x, x_dot, theta, theta_dot = self.state\n",
        "        costheta = math.cos(theta)\n",
        "        sintheta = math.sin(theta)\n",
        "        temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass\n",
        "        thetaacc = (self.gravity * sintheta - costheta * temp) / \\\n",
        "                    (self.length * (4.0/3.0 - self.masspole * costheta * costheta / self.total_mass))\n",
        "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
        "        #noise = self.addnoise(self.case) \n",
        "        x  = (x + self.tau * x_dot)\n",
        "        x_dot = (x_dot + self.tau * xacc)\n",
        "        theta = (theta + self.tau * theta_dot)#*(1 + noise)\n",
        "        theta_dot = (theta_dot + self.tau * thetaacc)\n",
        "        return (x, x_dot, theta, theta_dot)\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        assert self.action_space.contains(action), \"%r (%s) invalid\"%(action, type(action))\n",
        "        force = self.force_mag * float(action)\n",
        "        self.state = self.stepPhysics(force)\n",
        "        x, x_dot, theta, theta_dot = self.state         #true state\n",
        "\n",
        "        #adding measurement noisy to theta\n",
        "        noise = self.addnoise(self.case)\n",
        "        theta = theta * (1+noise)\n",
        "        noise = self.addnoise(self.case)\n",
        "        x = x * (1+noise)\n",
        "        noise = self.addnoise(self.case)\n",
        "        x_dot = x_dot*(1+noise)\n",
        "        noise = self.addnoise(self.case)\n",
        "        theta_dot = theta_dot*(1+noise)\n",
        "\n",
        "        output_state = (x, x_dot, theta, theta_dot) \n",
        "        output_state = np.array(output_state)  \n",
        "\n",
        "\n",
        "        done = x < -self.x_threshold \\\n",
        "            or x > self.x_threshold \\\n",
        "            or theta < -self.theta_threshold_radians \\\n",
        "            or theta > self.theta_threshold_radians\n",
        "        done = bool(done)\n",
        "\n",
        "        if not done:\n",
        "            reward = 1.0\n",
        "        elif self.steps_beyond_done is None:\n",
        "            # Pole just fell!\n",
        "            self.steps_beyond_done = 0\n",
        "            reward = 1.0\n",
        "        else:\n",
        "            if self.steps_beyond_done == 0:\n",
        "                logger.warn(\"You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\")\n",
        "            self.steps_beyond_done += 1\n",
        "            reward = 0.0\n",
        "\n",
        "        #return np.array(self.state), reward, done, {}\n",
        "        return output_state, reward, done, {}\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
        "        self.steps_beyond_done = None\n",
        "\n",
        "        #also reset the force\n",
        "        if self.origin_case<6:          #only model  noise\n",
        "            self.force_mag = 30.0*(1+self.addnoise(self.origin_case))\n",
        "        elif self.origin_case>9:    #both model and data noise\n",
        "            self.force_mag = 30.0*(1+self.addnoise(self.origin_case))\n",
        "        else:               #only data noise\n",
        "            self.force_mag = 30.0\n",
        "\n",
        "        return np.array(self.state)\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        if close:\n",
        "            if self.viewer is not None:\n",
        "                self.viewer.close()\n",
        "                self.viewer = None\n",
        "            return\n",
        "\n",
        "        screen_width = 600\n",
        "        screen_height = 400\n",
        "\n",
        "        world_width = self.x_threshold*2\n",
        "        scale = screen_width/world_width\n",
        "        carty = 100 # TOP OF CART\n",
        "        polewidth = 10.0\n",
        "        polelen = scale * 1.0\n",
        "        cartwidth = 50.0\n",
        "        cartheight = 30.0\n",
        "\n",
        "        if self.viewer is None:\n",
        "            from gym.envs.classic_control import rendering\n",
        "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
        "            l,r,t,b = -cartwidth/2, cartwidth/2, cartheight/2, -cartheight/2\n",
        "            axleoffset =cartheight/4.0\n",
        "            cart = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n",
        "            self.carttrans = rendering.Transform()\n",
        "            cart.add_attr(self.carttrans)\n",
        "            self.viewer.add_geom(cart)\n",
        "            l,r,t,b = -polewidth/2,polewidth/2,polelen-polewidth/2,-polewidth/2\n",
        "            pole = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n",
        "            pole.set_color(.8,.6,.4)\n",
        "            self.poletrans = rendering.Transform(translation=(0, axleoffset))\n",
        "            pole.add_attr(self.poletrans)\n",
        "            pole.add_attr(self.carttrans)\n",
        "            self.viewer.add_geom(pole)\n",
        "            self.axle = rendering.make_circle(polewidth/2)\n",
        "            self.axle.add_attr(self.poletrans)\n",
        "            self.axle.add_attr(self.carttrans)\n",
        "            self.axle.set_color(.5,.5,.8)\n",
        "            self.viewer.add_geom(self.axle)\n",
        "            self.track = rendering.Line((0,carty), (screen_width,carty))\n",
        "            self.track.set_color(0,0,0)\n",
        "            self.viewer.add_geom(self.track)\n",
        "\n",
        "        if self.state is None: return None\n",
        "\n",
        "        x = self.state\n",
        "        cartx = x[0]*scale+screen_width/2.0 # MIDDLE OF CART\n",
        "        self.carttrans.set_translation(cartx, carty)\n",
        "        self.poletrans.set_rotation(-x[2])\n",
        "        return self.viewer.render(return_rgb_array = mode=='rgb_array')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RlZ5-7Q972c"
      },
      "source": [
        "#RSSM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5FULoa998z1"
      },
      "source": [
        "\n",
        "\n",
        "class RSSM(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=32, output_size=4, state_size=32, device = 'cpu', mode = 'LSTM'):            #action, hidden and observation dim\n",
        "        super(RSSM, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.state_size = state_size\n",
        "\n",
        "        self.mode = mode\n",
        "        self.device = device\n",
        "\n",
        "                \n",
        "\n",
        "\n",
        "        if mode == 'RNN':\n",
        "            self.transition_RNN = nn.RNNCell(input_size = hidden_size, hidden_size = hidden_size)\n",
        "            #self.transition_RNN = nn.RNN(input_size = input_size, hidden_size = hidden_size)\n",
        "        elif mode == 'LSTM':\n",
        "            self.transition_RNN = nn.LSTMCell(input_size = hidden_size, hidden_size = hidden_size)\n",
        "            #self.transition_RNN = nn.LSTM(input_size = input_size, hidden_size = hidden_size)\n",
        "        elif mode == 'GRU':\n",
        "            self.transition_RNN = nn.GRUCell(input_size = hidden_size, hidden_size = hidden_size)\n",
        "            #self.transition_RNN = nn.GRU(input_size = input_size, hidden_size = hidden_size)\n",
        "\n",
        "\n",
        "\n",
        "        #linear layer converting the state + action\n",
        "        self.state_action_layer = nn.Sequential(\n",
        "            nn.Linear(self.state_size+self.input_size, self.hidden_size),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        #prior\n",
        "        #self.hidden_prior = nn.Sequential(\n",
        "        #    nn.Linear(self.hidden_size, self.hidden_size),\n",
        "        #    nn.ReLU()\n",
        "        #)\n",
        "        self.prior_mean = nn.Linear(self.hidden_size, self.state_size)\n",
        "        self.prior_sigma = nn.Linear(self.hidden_size, self.state_size)\n",
        "        self._min_stddev = 0.1\n",
        "\n",
        "        #poster\n",
        "        self.hidden_obs = nn.Sequential(\n",
        "            nn.Linear(self.hidden_size+self.output_size, self.hidden_size),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.poster = nn.Sequential(\n",
        "            nn.Linear(self.hidden_size, self.hidden_size),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.post_mean = nn.Linear(self.hidden_size, self.state_size)\n",
        "        self.post_sigma = nn.Linear(self.hidden_size, self.state_size)\n",
        "\n",
        "        #decoder\n",
        "        self.state_hidden = nn.Sequential(\n",
        "            nn.Linear(self.state_size+self.hidden_size, self.hidden_size),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        #self.obs = nn.Sequential(\n",
        "        #    nn.Linear(self.hidden_size, self.hidden_size),\n",
        "        #    nn.ReLU()\n",
        "        #)\n",
        "        self.obs_mean = nn.Linear(self.hidden_size, self.output_size)\n",
        "        self.obs_sigma = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "\n",
        "        #intial hidden encoder (take the fist observation as input and output an initial hidden state) \n",
        "        self.init_h = nn.Sequential(\n",
        "            nn.Linear(self.output_size, self.hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.hidden_size, self.hidden_size),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.loss_list = []\n",
        "\n",
        "\n",
        "    def prior(self, state, action, rnn_hidden, rnn_hidden_c=None):\n",
        "        \"\"\" \n",
        "        h_t+1 = f(h_t, s_t, a_t)\n",
        "        prior : p(s_t+1 | h_t+1)\n",
        "\n",
        "        state : [batch, state_dim]\n",
        "        action : [batch, action_dim]\n",
        "        rnn_hidden; [batch, rnn hidden dim]\n",
        "        \"\"\"\n",
        "        state_action = self.state_action_layer(torch.cat([state, action], dim = -1))         #[batch, hidden]\n",
        "        if self.mode == 'LSTM':\n",
        "            rnn_hidden, rnn_hidden_c = self.transition_RNN(state_action, (rnn_hidden, rnn_hidden_c))\n",
        "        else:\n",
        "            rnn_hidden = self.transition_RNN(state_action, rnn_hidden)      #[batch, hidden]\n",
        "        #hidden_prior = self.hidden_prior(rnn_hidden)            #[batch, hidden]\n",
        "        hidden_prior = rnn_hidden\n",
        "        prior_mean = self.prior_mean(hidden_prior)              #[batch, state]\n",
        "        #prior_sigma = F.softplus(self.prior_sigma(hidden_prior)) + self._min_stddev     #[batch, state]\n",
        "        prior_sigma = torch.exp(self.prior_sigma(hidden_prior))\n",
        "        if self.mode == 'LSTM':\n",
        "            return prior_mean, prior_sigma, rnn_hidden, rnn_hidden_c\n",
        "        else:\n",
        "            return prior_mean, prior_sigma, rnn_hidden       \n",
        "\n",
        "    def posterior(self, rnn_hidden, obs):\n",
        "        \"\"\"\n",
        "        posterior q(s_t | h_t, o_t)\n",
        "\n",
        "        rnn_hidden: [batch, hidden]\n",
        "        embedded_obs : [batch, output_dim]\n",
        "        \"\"\"\n",
        "        hidden_obs = self.hidden_obs(torch.cat([rnn_hidden, obs], dim = -1))        #[batch, hidden]\n",
        "        poster = self.poster(hidden_obs)            #[batch, hidden]\n",
        "        poster_mean = self.post_mean(poster)        #[batch, state]\n",
        "        #poster_sigma = F.softplus(self.post_sigma(poster)) + self._min_stddev     #[batch, state]\n",
        "        poster_sigma = torch.exp(self.post_sigma(poster))\n",
        "\n",
        "        return poster_mean, poster_sigma\n",
        "\n",
        "    def obs_model(self, state, rnn_hidden):\n",
        "        \"\"\"\n",
        "        p(o_t | s_t, h_t)\n",
        "        \"\"\"\n",
        "        state_hidden = self.state_hidden(torch.cat([state, rnn_hidden], dim = -1))      #[batch, hidden]\n",
        "        #obs = self.obs(state_hidden)            #[batch, hidden]\n",
        "        obs = state_hidden\n",
        "        obs_mean = self.obs_mean(obs)           #[batch, output_size]\n",
        "        #obs_sigma = F.softplus(self.obs_sigma(obs))+self._min_stddev         #[batch, output_size]\n",
        "        obs_sigma = torch.exp(self.obs_sigma(obs))\n",
        "\n",
        "        return obs_mean, obs_sigma\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, X, A, beta=1,print_output=False):\n",
        "        \"\"\"\n",
        "        Likelihood objective function for a given trajectory (change to batched verision later)\n",
        "        X: data matrix of shape [seq_length, batch, output_size]]      (we only feed one trajectory here for testing)\n",
        "        A: data matrix of action [seq_length-1, batch]\n",
        "        \"\"\"\n",
        "        assert X.size(0) == A.size(0)+1, print('the seq length of X and A are wrong')\n",
        "        kl_loss = 0             #KL divergence term\n",
        "        Ell_loss = 0             #expected log likelihood term\n",
        "        batch_size = X.size(1)\n",
        "\n",
        "        if len(X.size()) != 3:\n",
        "            print('The input data matrix should be the shape of [seq_length, batch_size, input_dim]')\n",
        "        \n",
        "        X = X.to(self.device)\n",
        "        A = A.to(self.device)\n",
        "\n",
        "        #container\n",
        "        states = torch.zeros(A.size(0), A.size(1), self.state_size).to(self.device)         #[seq-1, batch, state]\n",
        "        rnn_hiddens = torch.zeros(A.size(0), A.size(1), self.hidden_size).to(self.device)   #[seq-1, batch, hidden]\n",
        "\n",
        "        #initialising state and rnn hidden state\n",
        "        #state = torch.zeros(X.size(1), self.state_size).to(self.device)\n",
        "        rnn_hidden = self.init_h(X[0]).to(self.device)          #[batch, hidden]\n",
        "        if self.mode == 'LSTM':\n",
        "            rnn_hidden_c = torch.zeros_like(rnn_hidden).to(self.device)     #[batch, hidden]\n",
        "\n",
        "        #temp_prior = self.hidden_prior(rnn_hidden)      #[batch, state]\n",
        "        temp_prior = rnn_hidden\n",
        "        prior_mean = self.prior_mean(temp_prior)        #[batch, state]\n",
        "        prior_sigma = torch.exp(self.prior_sigma(temp_prior))       #[batch, state]\n",
        "        state = self.reparametrise(prior_mean, prior_sigma)     #[batch, state]\n",
        "\n",
        "        #rnn_hidden = torch.zeros(X.size(1), self.hidden_size).to(self.device)\n",
        "\n",
        "        \n",
        "        #emission_mean = X[0]\n",
        "        for t in range(1, X.size()[0]):          #for each time step, compute the free energy for each batch of data (start from the second hid state)\n",
        "            if self.mode == 'LSTM':\n",
        "                next_state_prior_m, next_state_prior_sigma, rnn_hidden, rnn_hidden_c= self.prior(state, A[t-1].unsqueeze(-1), \n",
        "                                                                                                 rnn_hidden, rnn_hidden_c)\n",
        "            else:\n",
        "                next_state_prior_m, next_state_prior_sigma, rnn_hidden = self.prior(state, A[t-1].unsqueeze(-1), rnn_hidden)\n",
        "\n",
        "            next_state_post_m, next_state_post_sigma = self.posterior(rnn_hidden, X[t])\n",
        "            state = self.reparametrise(next_state_post_m, next_state_post_sigma)        #[batch, state_size]\n",
        "            states[t-1] = state\n",
        "            rnn_hiddens[t-1] = rnn_hidden\n",
        "            next_state_prior = Normal(next_state_prior_m, next_state_prior_sigma)\n",
        "            next_state_post = Normal(next_state_post_m, next_state_post_sigma)\n",
        "\n",
        "            #kl = kl_divergence(next_state_prior, next_state_post).sum(dim=1)        #[batch]\n",
        "            kl = kl_divergence(next_state_post, next_state_prior).sum(dim=1)        #[batch]\n",
        "\n",
        "            kl_loss += kl.mean()\n",
        "        kl_loss /= A.size(0)\n",
        "\n",
        "        #compute nll\n",
        "\n",
        "        #flatten state\n",
        "        flatten_states = states.view(-1, self.state_size)\n",
        "        flatten_rnn_hiddens = rnn_hiddens.view(-1, self.hidden_size)\n",
        "        flatten_x_mean, flatten_x_sigma = self.obs_model(flatten_states, flatten_rnn_hiddens)\n",
        "\n",
        "\n",
        "        nll = self.batched_gaussian_ll(flatten_x_mean, flatten_x_sigma, X[1:,:,:].reshape(-1, self.output_size))\n",
        "        nll = nll.mean()\n",
        "\n",
        "        FE = nll - kl_loss\n",
        "\n",
        "        if print_output:\n",
        "            #print('ELL loss=', Ell_loss, 'KL loss=', kl_loss)\n",
        "            print('Free energy of this batch = {}. Nll loss = {}. KL div = {}.'.format(float(FE.data)\n",
        "                                                                        , float(nll.data), \n",
        "                                                                        float(kl_loss.data)))\n",
        "  \n",
        "\n",
        "        return FE, nll, kl_loss\n",
        "\n",
        "\n",
        "    def mc_predict(self, initial_obs, actions, mean_obs = False):\n",
        "        \"\"\" \n",
        "        initial_obs : [1, output_dim]\n",
        "        actions: [seq-1, 1, action_dim]\n",
        "        \"\"\"\n",
        "\n",
        "        initial_obs = initial_obs.to(self.device)\n",
        "        actions = actions.to(self.device)\n",
        "\n",
        "        total_list = []\n",
        "\n",
        "        time_step = actions.size(0) \n",
        "\n",
        "        total_list = []\n",
        "                                 #[1, output]\n",
        "        for i in range(200):\n",
        "            temp_pred = []\n",
        "            #container\n",
        "            #states = torch.zeros(actions.size(0), actions.size(1), self.state_size).to(self.device)         #[seq-1, 1, state]\n",
        "            #rnn_hiddens = torch.zeros(actions.size(0), actions.size(1), self.hidden_size).to(self.device)   #[seq-1, 1, hidden]\n",
        "\n",
        "            #initialising state and rnn hidden state\n",
        "            #state = torch.zeros(initial_obs.size(0), self.state_size).to(self.device)             #[1, state]\n",
        "            rnn_hidden = self.init_h(initial_obs)                                       #[1, hidden]\n",
        "            if self.mode == 'LSTM':\n",
        "                rnn_hidden_c = torch.zeros_like(rnn_hidden)         #[1, hidden]\n",
        "            \n",
        "            #temp_prior = self.hidden_prior(rnn_hidden)      #[1, state]\n",
        "            temp_prior = rnn_hidden\n",
        "            prior_mean = self.prior_mean(temp_prior)\n",
        "            prior_sigma = torch.exp(self.prior_sigma(temp_prior))\n",
        "            state = self.reparametrise(prior_mean, prior_sigma)\n",
        "\n",
        "            #x_sample = initial_obs      #[1, output_dim]\n",
        "            for t in range(time_step):\n",
        "                if self.mode == 'LSTM':\n",
        "                    next_state_prior_m, next_state_prior_sigma, rnn_hidden, rnn_hidden_c= self.prior(state, \n",
        "                                                                                                     actions[t], \n",
        "                                                                                                     rnn_hidden, rnn_hidden_c)\n",
        "                else:\n",
        "                    next_state_prior_m, next_state_prior_sigma, rnn_hidden = self.prior(state, actions[t], rnn_hidden)\n",
        "                    \n",
        "                state = self.reparametrise(next_state_prior_m, next_state_prior_sigma)\n",
        "\n",
        "\n",
        "                #next_state_post_m, next_state_post_sigma = self.posterior(rnn_hidden, x_sample)\n",
        "                #state = self.reparametrise(next_state_post_m, next_state_post_sigma)        #[batch, state_size]\n",
        "\n",
        "                x_mean, x_sigma = self.obs_model(state, rnn_hidden)      \n",
        "                if mean_obs:\n",
        "                    x_sample = x_mean\n",
        "                else:\n",
        "                    x_sample = self.reparametrise(x_mean, x_sigma)          #[1, output_dim]\n",
        "\n",
        "                temp_pred.append(x_sample.unsqueeze(0))                 #list of shape [1,1,output]\n",
        "            temp_pred_vec = torch.cat(temp_pred, dim = 0)       #[seq-1, 1, output]\n",
        "\n",
        "            total_list.append(temp_pred_vec.unsqueeze(-1))      #list of shape [seq-1, 1, output, 1]\n",
        "        total_list = torch.cat(total_list, dim = -1)            #[seq-1, 1, output, 200]\n",
        "        mean = total_list.mean(dim = -1)                    #[seq-1, 1, output]\n",
        "        std = total_list.std(dim = -1)                      #[seq-1, 1, output]\n",
        "\n",
        "        return mean, std\n",
        "    \n",
        "    def imagine(self, init_x, control_f, horizon, plan, mean_obs = False):\n",
        "        \"\"\"\n",
        "        init_x : [batch, output]\n",
        "        \"\"\"\n",
        "        init_x = init_x.to(self.device)\n",
        "        rnn_hidden = self.init_h(init_x)\n",
        "        if self.mode == 'LSTM':\n",
        "            rnn_hidden_c = torch.zeros_like(rnn_hidden).to(self.device)\n",
        "\n",
        "        #temp_prior = self.hidden_prior(rnn_hidden)      #[1, state]\n",
        "        temp_prior = rnn_hidden\n",
        "        prior_mean = self.prior_mean(temp_prior)\n",
        "        prior_sigma = torch.exp(self.prior_sigma(temp_prior))\n",
        "        state = self.reparametrise(prior_mean, prior_sigma)\n",
        "\n",
        "        x_sample = init_x\n",
        "        pred = []\n",
        "        action_log_prob_list = []\n",
        "        for t in range(horizon):\n",
        "            if plan == 'pg':\n",
        "                action_samples, action_log_prob = control_f(x_sample)\n",
        "                action_log_prob_list.append(action_log_prob.unsqueeze(0))           #[1, batch, 1]\n",
        "\n",
        "            elif plan == 'rp':\n",
        "                action_samples, _= control_f(x_sample)          #[batch, 1]\n",
        "                action_log_prob_list = 0\n",
        "            else:\n",
        "                raise NotImplementedError\n",
        "            \n",
        "            if self.mode == 'LSTM':\n",
        "                next_state_prior_m, next_state_prior_sigma, rnn_hidden, rnn_hidden_c= self.prior(state, \n",
        "                                                                                                    action_samples, \n",
        "                                                                                                    rnn_hidden, rnn_hidden_c)   \n",
        "            else:\n",
        "                next_state_prior_m, next_state_prior_sigma, rnn_hidden = self.prior(state, action_samples, rnn_hidden)\n",
        "            \n",
        "            state = self.reparametrise(next_state_prior_m, next_state_prior_sigma)\n",
        "\n",
        "            x_mean, x_sigma = self.obs_model(state, rnn_hidden)      \n",
        "            if mean_obs:\n",
        "                x_sample = x_mean\n",
        "            else:\n",
        "                x_sample = self.reparametrise(x_mean, x_sigma)          #[1, output_dim]\n",
        "            pred.append(x_sample.unsqueeze(0))\n",
        "\n",
        "        if plan == 'pg':\n",
        "            action_log_prob_list = torch.cat(action_log_prob_list)  #[seq-1, batch, 1]\n",
        "\n",
        "        return torch.cat(pred), action_log_prob_list\n",
        "\n",
        "    def validate_by_imagination(self, init_x, control_f, plan, mean_obs = False):\n",
        "        \"\"\"\n",
        "        Perform planning on learnt model as opposed to real dynamics\n",
        "        \"\"\"\n",
        "        init_x = init_x.to(self.device)\n",
        "        rnn_hidden = self.init_h(init_x)\n",
        "        if self.mode == 'LSTM':\n",
        "            rnn_hidden_c = torch.zeros_like(rnn_hidden).to(self.device)\n",
        "\n",
        "        #temp_prior = self.hidden_prior(rnn_hidden)      #[1, state]\n",
        "        temp_prior = rnn_hidden\n",
        "        prior_mean = self.prior_mean(temp_prior)\n",
        "        prior_sigma = torch.exp(self.prior_sigma(temp_prior))\n",
        "        state = self.reparametrise(prior_mean, prior_sigma)\n",
        "\n",
        "        x_sample = init_x\n",
        "        pred = []\n",
        "        action_log_prob_list = []\n",
        "        reward = 0\n",
        "        iter = 0\n",
        "\n",
        "        while True:\n",
        "            action_samples, _= control_f(x_sample)          #[batch, 1]\n",
        "\n",
        "            if self.mode == 'LSTM':\n",
        "                next_state_prior_m, next_state_prior_sigma, rnn_hidden, rnn_hidden_c= self.prior(state, action_samples, rnn_hidden, rnn_hidden_c)   \n",
        "            else:\n",
        "                next_state_prior_m, next_state_prior_sigma, rnn_hidden = self.prior(state, action_samples, rnn_hidden)\n",
        "        \n",
        "            state = self.reparametrise(next_state_prior_m, next_state_prior_sigma)\n",
        "\n",
        "            x_mean, x_sigma = self.obs_model(state, rnn_hidden)      \n",
        "            if mean_obs:\n",
        "                x_sample = x_mean\n",
        "            else:\n",
        "                x_sample = self.reparametrise(x_mean, x_sigma)          #[1, output_dim]\n",
        "\n",
        "            reward += 1\n",
        "            iter += 1\n",
        "\n",
        "            done = x_sample[:,0] < -2.4 \\\n",
        "                or x_sample[:,0] > 2.4 \\\n",
        "                or x_sample[:,2] < -12 * 2 * math.pi / 360 \\\n",
        "                or x_sample[:,2] > 12 * 2 * math.pi / 360 \\\n",
        "                or iter >= 200\n",
        "            done = bool(done)\n",
        "            if done:\n",
        "                break   \n",
        "            \n",
        "        return reward\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def reparametrise(self, mean, sigma):\n",
        "        \"\"\"\n",
        "        sigma should have the same shape as mean (no correaltion)\n",
        "        \"\"\"\n",
        "        eps = torch.rand_like(sigma).normal_()\n",
        "        eps = eps.to(self.device)\n",
        "        return mean + sigma*eps\n",
        "\n",
        "\n",
        "    def batched_gaussian_ll(self, mean, sigma, x):\n",
        "        \"\"\"\n",
        "        log-likelihood of batched observation\n",
        "        mean : shape [batch, output_size]\n",
        "        sigma  : shape [batch, output_size]   (diagonal covariance)\n",
        "        x    : shape [batch, output_size]\n",
        "        the shape of final result is [batch, ]\n",
        "        \"\"\"\n",
        "        #mean = mean.to(self.device)\n",
        "        #sigma = sigma.to(self.device)\n",
        "        if 0 in sigma:\n",
        "            #sigma = sigma + 1e-10\n",
        "            print('Zero occurs in diagonal sigma matrix. (batched gaussian ll)')\n",
        "        if 0 in sigma**2:\n",
        "            print('Zero occurs after squaring sigma matrix. (batched gaussian ll)')\n",
        "\n",
        "        inv_diag_cov = self.diagonalise(1/(sigma**2), batch=True)              #a 2d batched matrix----> 3d batched diagonal tensor      \n",
        "\n",
        "\n",
        "        exp = ((x - mean).unsqueeze(-2)) @ inv_diag_cov @ ((x-mean).unsqueeze(-1))      #\n",
        "        exp = exp.squeeze()         #[batch]   \n",
        "        #print(exp) \n",
        "\n",
        "        if 0 in torch.prod(sigma**2, dim = -1):\n",
        "            print('Zero occurs when calculating determinant of diagonal covariance. (batched gaussian ll)')\n",
        "\n",
        "\n",
        "        logdet = torch.sum(2 * torch.log(sigma) , dim = -1)\n",
        "        #logdet = torch.log(torch.prod(sigma**2, dim = -1))         #product of all diagonal variance for each batch, shape [batch]\n",
        "        #print('logdet=', logdet)\n",
        "        n = mean.size()[-1]\n",
        "\n",
        "\n",
        "        return -(n/2) * np.log(2*np.pi) - 0.5*logdet - 0.5 * exp        #need double checking\n",
        "\n",
        "     \n",
        "    \n",
        "    def diagonalise(self, input, batch):\n",
        "        \"\"\"\n",
        "        if input a vector, return a diagonal matrix\n",
        "        if input a non-batched 2d matrix, return a diagonal matrix, eg: [[1,2],[3,4]] ---> diag([1,2,3,4])\n",
        "        if input a batched 2d matrix, return a batched diagonal matrix\n",
        "        if input a 3d batched tensor, return a batched diagonal tensor\n",
        "        \"\"\"\n",
        "        if len(input.size())==1:\n",
        "            return torch.diag(input)\n",
        "        if len(input.size())==2:\n",
        "            if not batch:\n",
        "                return torch.diag(vec(input))\n",
        "            else:\n",
        "                bdiag = torch.Tensor().to(self.device)\n",
        "                for i in range(input.size()[0]):\n",
        "                    bdiag = torch.cat((bdiag, torch.diag(input[i]).unsqueeze(0)), axis = 0)\n",
        "                return bdiag\n",
        "\n",
        "        if len(input.size())==3 and batch:\n",
        "            bdiag = torch.Tensor()\n",
        "            for i in range(input.size()[0]):\n",
        "                bdiag = torch.cat((bdiag, torch.diag(vec(input[i])).unsqueeze(0)), axis = 0)\n",
        "    \n",
        "            return bdiag\n",
        "        else:\n",
        "            print('Dimension of inpout tensor should only be 1,2,3.')\n",
        "\n",
        "\n",
        "\n",
        "    def print_loss(self):\n",
        "        return self.loss_list\n",
        "\n",
        "\n",
        "    def print_params(self):\n",
        "\n",
        "        pass\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO4Vv1fp-JaE"
      },
      "source": [
        "#Deterministic controller"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHVyWB0l-Lr5"
      },
      "source": [
        "class controller(nn.Module):\n",
        "    def __init__(self, action_dim=1, state_dim=4, deterministic = True,device = 'cuda'):\n",
        "        super(controller, self).__init__()\n",
        "        self.action_dim = action_dim\n",
        "        controller_hid = 16\n",
        "        self.state_dim  = state_dim\n",
        "        init_w = 1e-3\n",
        "\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(self.state_dim,controller_hid),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(controller_hid, controller_hid),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(controller_hid, action_dim)\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #if not deterministic:\n",
        "        #    self.std = 0.1\n",
        "        \n",
        "        #self.deterministic = deterministic\n",
        "        \n",
        "        #nn.Linear(self.state_dim, self.action_dim)        #output a p_logits for action 1\n",
        "        self.device = device\n",
        "        self.optimiser = torch.optim.Adam(self.parameters(), lr = 0.001)\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"\n",
        "        Given states input [batch, state_dim],\n",
        "        \"\"\"\n",
        "        state = state.to(self.device)\n",
        "        #x = F.relu(self.linear1(state))\n",
        "        #x = F.relu(self.linear2(x))\n",
        "        a = self.linear(state)\n",
        "        a = torch.tanh(a)\n",
        "        log_pi = 0\n",
        "\n",
        "        return a, log_pi\n",
        "\n",
        "\n",
        "        #out_mean = self.linear(state)         #[batch, action_dim]\n",
        "        \n",
        "        #if not self.deterministic:\n",
        "        #    eps = torch.rand_like(out_mean).normal_().to(self.device)           #[batch, action_dim]\n",
        "        #    out = out_mean + self.std * eps\n",
        "        #else:\n",
        "        #    out = out_mean\n",
        "\n",
        "        #if len(out.shape) == 1:\n",
        "        #    out = torch.clamp(out, -1, 1)\n",
        "        #else:\n",
        "        #    out = torch.clamp(out[:,0], -1, 1).unsqueeze(1)             #[1, batch, 1]\n",
        "\n",
        "        #return torch.tanh(out)\n",
        "\n",
        "\n",
        "        #if len(out.shape) == 1:\n",
        "        #    return torch.clamp(out, -1, 1)\n",
        "        #else:\n",
        "        #    clamp_out = torch.clamp(out[:, 0], -1, 1).unsqueeze(-1)\n",
        "        #    return clamp_out\n",
        "    \n",
        "    def make_decision(self, state, behaviour_uncertainty):\n",
        "        \"\"\"\n",
        "        given a state [batch, state_dim], output a action\n",
        "        \"\"\"\n",
        "        state = state.to(self.device)\n",
        "        #x = F.relu(self.linear1(state))\n",
        "        #x = F.relu(self.linear2(x))\n",
        "        a = self.linear(state)\n",
        "        a = torch.tanh(a)\n",
        "\n",
        "        return a.detach()\n",
        "\n",
        "        #out_mean = self.linear(state)\n",
        "        #if not self.deterministic and behaviour_uncertainty:\n",
        "        #    eps = torch.rand_like(out_mean).normal_().to(self.device) \n",
        "        #    out = out_mean + self.std * eps\n",
        "        #else:\n",
        "        #    out = out_mean\n",
        "        #if len(out.shape) == 1:\n",
        "\n",
        "        #    out = torch.clamp(out, -1, 1)\n",
        "        #else:\n",
        "        #    out = torch.clamp(out[:,0], -1, 1)\n",
        "        #return out.detach()\n",
        "        #return torch.tanh(out)\n",
        "    def pg_train(self, num_epoch, initial_state, horizon, cost_f, model_imagine_f, w_uncertainty, e_uncertainty,gamma = 0.95):\n",
        "        \"\"\"\n",
        "        initial_state : [batch, state_dim]\n",
        "\n",
        "        \"\"\"\n",
        "        loss_list = []\n",
        "        num_particle = 100\n",
        "        initial_state = initial_state.expand(num_particle, self.state_dim)\n",
        "\n",
        "        for e in range(num_epoch):\n",
        "            output_matrix, action_log_prob_matrix = model_imagine_f(initial_state, self.forward, horizon, plan = 'pg',\n",
        "                                                                    W_uncertainty = w_uncertainty, e_uncertainty = e_uncertainty)\n",
        "            cost = cost_f(output_matrix).detach()               #[seq-1, batch, 1]  \n",
        "            \n",
        "            cost = cost * torch.tensor([gamma**(t+1) for t in range(cost.size(0))]).unsqueeze(-1).unsqueeze(-1).to(self.device)\n",
        "\n",
        "            #baseline = torch.mean(cost, dim = 0).unsqueeze(0)       #[1, batch, 1]\n",
        "            #cost = cost - torch.mean(cost, dim = 0).unsqueeze(0)\n",
        "            #loss = ((cost-baseline) * action_log_prob_matrix).sum(0)\n",
        "            loss = cost.sum(0) * action_log_prob_matrix.sum(0) \n",
        "\n",
        "            loss = loss.sum()\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(self.parameters(), 5)\n",
        "\n",
        "            self.optimiser.step()\n",
        "            loss_list.append(loss.item())\n",
        "            if e%50 == 0:\n",
        "                print('Epoch = {}; Policy gradient training loss = {}'.format(e, loss.item()))\n",
        "        return loss_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    def rp_train(self, num_epoch, num_particle,initial_state, horizon , cost_f, model_imagine_f, mean_obs, gamma = 0.9):\n",
        "        \"\"\"\n",
        "        From an initial state, use mode imagination function to make prediction of the next state accordin to the action proposed by\n",
        "        the controller, we fixed the horizon and compute the total reward of the trajectory, from which the gradient w.r.t policy \n",
        "        parameters is taken.\n",
        "        inital_state: [batch, output_dim]\n",
        "        \"\"\"\n",
        "        loss_list = []\n",
        "        num_particle = num_particle\n",
        "        initial_state = initial_state.expand(num_particle, self.state_dim)\n",
        "\n",
        "        cost_mean_list = []\n",
        "        cost_std_list = []\n",
        "        \n",
        "        for e in range(num_epoch):\n",
        "            self.optimiser.zero_grad()\n",
        "            output_matrix, action_matrix= model_imagine_f(initial_state, self.forward, horizon, plan = 'rp', mean_obs = mean_obs) \n",
        "            \n",
        "            #print('output matrix dim = ',output_matrix.shape)\n",
        "            self.action_matrix = action_matrix\n",
        "            self.temp_output_matrix = torch.cat([initial_state.unsqueeze(0).to(self.device), output_matrix], dim = 0)\n",
        "\n",
        "            cost = cost_f(output_matrix)                 #[seq-1, batch, 1]  \n",
        "\n",
        "            mean_cost = cost.data.sum(0).mean(0)      #[]\n",
        "            std_cost = cost.data.sum(0).std(0)\n",
        "            cost_mean_list.append(mean_cost)\n",
        "            cost_std_list.append(std_cost)\n",
        "\n",
        "            #multiply by discount factor\n",
        "            #cost = cost *  ((torch.arange(cost.size(0)+1,1,-1).float()).unsqueeze(-1).unsqueeze(-1)/cost.size(0)\n",
        "            #                    ).expand(cost.shape).float().to(self.device)\n",
        "            \n",
        "            cost = cost * torch.tensor([gamma**(t+1) for t in range(cost.size(0))]).unsqueeze(-1).unsqueeze(-1).to(self.device)\n",
        "\n",
        "            loss = cost.sum()      #[batch, 1]\n",
        "            #loss = torch.exp(action_log_prob_matrix.sum(0)) * cost.sum(0)\n",
        "            #loss = (cost * action_log_prob_matrix).sum(0)                 #[batch, 1]\n",
        "            loss.backward()\n",
        "\n",
        "            nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "            self.optimiser.step()\n",
        "            loss_list.append(loss.item())\n",
        "            #print('policy loss = {}', loss.item())\n",
        "            #if e%10 == 0:\n",
        "            #print('Epoch = {}; Policy gradient training loss = {}; Cost: mean {} std {}.'.format(e, loss.item()/num_particle,\n",
        "            #                                                                                           mean_cost.item(), std_cost.item()))\n",
        "\n",
        "\n",
        "        return loss_list, torch.cat(cost_mean_list), torch.cat(cost_std_list)\n",
        "\n",
        "    def rp_validate(self, num_particle, initial_state, horizon, cost_f, model_imagine_f, mean_obs):\n",
        "        initial_state = initial_state.expand(num_particle, self.state_dim)\n",
        "        output_matrix, action_matrix = model_imagine_f(initial_state, self.forward, horizon, plan = 'rp', mean_obs = mean_obs)\n",
        "\n",
        "        cost = cost_f(output_matrix)\n",
        "        mean_cost = cost.data.sum(0).mean(0)\n",
        "        return mean_cost.item()\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTordXwK-U32"
      },
      "source": [
        "#Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XwoC2Or-V_b"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, env_case, state_dim = 4, action_dim = 1, mode = 'LSTM',device='cuda', rand_seed = 1):\n",
        "        \n",
        "\n",
        "        self.env =  CartPoleModEnv(case = env_case)\n",
        "        self.env_case = env_case\n",
        "\n",
        "        #self.env = gym.make('CartPoleMod-v2')\n",
        "\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.device = device\n",
        "        self.observations_list = []\n",
        "        self.actions_list= []\n",
        "\n",
        "\n",
        "\n",
        "        self.model = RSSM(input_size=action_dim, hidden_size=32, output_size=4, state_size= 32, device=device,\n",
        "                          mode = mode).to('cuda')\n",
        "\n",
        "\n",
        "        self.model_optimiser = torch.optim.Adam(self.model.parameters(), lr = 0.001)\n",
        "        self.model_training_loss_list = []\n",
        "\n",
        "        self.policy = controller(action_dim, state_dim, device).to(device)\n",
        "\n",
        "        #np.random.seed(rand_seed)\n",
        "        #self.env.seed(rand_seed)\n",
        "        #torch.manual_seed(rand_seed)\n",
        "    \n",
        "    def env_reset(self):\n",
        "        self.env = ContinuousCartPoleEnv(case = self.env_case)\n",
        "\n",
        "    \n",
        "    def env_rollout(self, if_remember, plan, behaviour_uncertainty):\n",
        "        \"\"\"\n",
        "        interaction with the environment using the current policy. \n",
        "        \"\"\"\n",
        "        done = False\n",
        "        state = self.env.reset()\n",
        "        total_reward = 0\n",
        "        i = 0\n",
        "        temp_obs_list = []\n",
        "        temp_actions_list = []\n",
        "\n",
        "        if if_remember:\n",
        "            temp_obs_list.append(torch.tensor(state))\n",
        "        \n",
        "        while not done:\n",
        "            i+=1\n",
        "            \n",
        "            if plan == 'random':\n",
        "                action = self.env.action_space.sample()\n",
        "            elif plan == 'pg' or 'rp':\n",
        "\n",
        "                state_tensor = torch.tensor(np.vstack(state)).float().squeeze()\n",
        "\n",
        "\n",
        "                action = self.policy.make_decision(state_tensor.to(self.device), behaviour_uncertainty)\n",
        "                action = action.detach().cpu().numpy()\n",
        "\n",
        "                \n",
        "            else:\n",
        "                raise NotImplementedError\n",
        "            #print('action = ', action)\n",
        "\n",
        "            #next_state, reward, done, _  = self.env.step(int(action))\n",
        "            next_state, reward, _, _  = self.env.step(action)\n",
        "\n",
        "            #print('next state = ', next_state)\n",
        "\n",
        "            done = next_state[0] < -2.4 \\\n",
        "                or next_state[0] > 2.4 \\\n",
        "                or next_state[2] < -12 * 2 * math.pi / 360 \\\n",
        "                or next_state[2] > 12 * 2 * math.pi / 360 \\\n",
        "                or i >= 200\n",
        "            \n",
        "            #print('done = ', done)\n",
        "\n",
        "            if if_remember:\n",
        "\n",
        "                temp_obs_list.append(torch.tensor(np.vstack(next_state)).squeeze())\n",
        "\n",
        "                temp_actions_list.append(torch.tensor(action).float())\n",
        "            \n",
        "            state = next_state\n",
        "            total_reward += 1\n",
        "\n",
        "            #if total_reward > 200:\n",
        "            #    break\n",
        "\n",
        "        if if_remember:\n",
        "            self.observations_list.append(torch.stack(temp_obs_list).float())       #list of shape [seq, output]\n",
        "            self.actions_list.append(torch.stack(temp_actions_list).float())        #list of shape [seq-1, 1]\n",
        "\n",
        "        return total_reward\n",
        "    \n",
        "    def model_learning(self, num_epoch, num_batch):\n",
        "        \"\"\"\n",
        "        perform model leanring using data self.observation_list and self.actions_list; since the data has variable length, one could \n",
        "        try truncate the data into same length or pack_padded_sequence, but here we would simply train each single sample in a batch,\n",
        "        and during each epoch, the parameter is only updated once using part of the dataset\n",
        "        num_epoch : number of training epoch\n",
        "        num_batch: this is actually number of samples we want the model to be trained on during each epoch\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "        for e in range(num_epoch):\n",
        "            self.model_optimiser.zero_grad()\n",
        "\n",
        "            idx = np.random.choice(len(self.observations_list), num_batch)\n",
        "            trun_obs = truncate_sequence([self.observations_list[i] for i in idx], batch_first = False)\n",
        "            trun_actions = truncate_sequence([self.actions_list[j] for j in idx], batch_first = False)\n",
        "\n",
        "            b_FE, b_LL, b_KL = self.model(trun_obs, trun_actions.squeeze())\n",
        "            loss = -b_FE\n",
        "            loss.backward()\n",
        "\n",
        "            nn.utils.clip_grad_norm_(self.model.parameters(), 1)\n",
        "            #for i in idx:\n",
        "            #    training_obs = self.observations_list[i].unsqueeze(1)       #[seq, 1, output]\n",
        "            #    training_actions = self.actions_list[i]                      #[seq-1, 1]\n",
        "\n",
        "            #    pred = self.model(training_obs[0,:,:], training_actions.unsqueeze(-1))\n",
        "            #    loss = self.mseloss(torch.cat(pred).unsqueeze(1), training_obs[1:, :, :].to(self.device))\n",
        "            #    temp_loss += loss\n",
        "            #temp_loss.backward()\n",
        "\n",
        "            self.model_optimiser.step()\n",
        "            self.model_training_loss_list.append(loss.item())\n",
        "\n",
        "            if e%1000 == 0:\n",
        "                print('Epoch{}; FE = {}; LL = {}; KL = {}.'.format(e, b_FE.item(), b_LL.item(), b_KL.item()))\n",
        "\n",
        "\n",
        "\n",
        "    def cost(self, state):\n",
        "        \"\"\"\n",
        "        cost = 5*angle^2 + position^2\n",
        "        state : [seq, batch, output]\n",
        "        return [seq, batch, 1]\n",
        "        \"\"\"\n",
        "        return (5*state[:,:,2]**2 + state[:,:,0]**2).unsqueeze(-1)      #[seq, batch, 1]\n",
        "\n",
        "    '''\n",
        "    def cost(self, states, sigma=0.25):\n",
        "        \"\"\"\n",
        "        states : [seq, batch, output]\n",
        "        return : [seq, batch, 1]\n",
        "        \"\"\"\n",
        "        l = 0.6\n",
        "        seq_length = states.size(0)\n",
        "        batch_size = states.size(1)\n",
        "        feature_dim = states.size(-1)\n",
        "        \n",
        "        goal = Variable(torch.FloatTensor([0.0, l])).unsqueeze(0).unsqueeze(0).expand(seq_length,1, 2).to(self.device)     #[seq, 1,2]\n",
        "\n",
        "        # Cart position\n",
        "        cart_x = states[:,:, 0]         #[seq, batch]\n",
        "        # Pole angle\n",
        "        thetas = states[:,:,2]          #[seq, bnatch]\n",
        "        # Pole position\n",
        "        x = torch.sin(thetas)*l         #[seq, batch]\n",
        "        y = torch.cos(thetas)*l\n",
        "        positions = torch.stack([cart_x + x, y], -1)             #[seq, batch, 2]\n",
        "\n",
        "        \n",
        "        squared_distance = torch.sum((goal - positions)**2, -1).unsqueeze(-1)          #[]\n",
        "\n",
        "        squared_sigma = sigma**2\n",
        "        cost = 1 - torch.exp(-0.5*squared_distance/squared_sigma)\n",
        "        \n",
        "        return cost\n",
        "    '''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def policy_learning(self, imagine_num, num_particle,num_epoch, batch_size, horizon, plan, mean_obs,\n",
        "                        plot=False):\n",
        "        \"\"\"\n",
        "        we utilise the current learned model to do policy learning on imagined data\n",
        "        num_epoch : number of epochs we want to run our policy gradient for\n",
        "        batch_size : number of samples we want to train the policy on/ number of initial states\n",
        "\n",
        "        we creat batch_size number of initial state, the model then rollout for a fixed length(horizon), the sum of cost for each \n",
        "        imagined trajectory is computed, from which the gradient is taken w.r.t the policy parameters\n",
        "        \"\"\"\n",
        "        #creat inital states \n",
        "        for i in range(imagine_num):\n",
        "            #initial_state = []\n",
        "            #for b in range(batch_size):\n",
        "            #    init_x = self.env.reset()\n",
        "            #    initial_state.append(torch.tensor(init_x).float())\n",
        "            #initial_state = torch.stack(initial_state)          #[batch, output]\n",
        "            initial_state = torch.tensor(self.env.reset()).unsqueeze(0).float()         #[1, output]\n",
        "            if plot:\n",
        "                initial_state = torch.zeros_like(initial_state)\n",
        "            #initial_state = torch.zeros_like(initial_state)\n",
        "            #initial_state = torch.tensor(np.array([ 0.04263216,  0.00452452, -0.03763419, -0.03992425])).float().unsqueeze(0)\n",
        "            #learn the policy parameter using current model\n",
        "\n",
        "            model_f = self.model.imagine\n",
        "\n",
        "            if plan == 'pg':\n",
        "                policy_train_loss = self.policy.pg_train(num_epoch, initial_state, horizon , self.cost, model_f, gamma=1)\n",
        "            elif plan == 'rp':\n",
        "                policy_train_loss = self.policy.rp_train(num_epoch, num_particle,initial_state, horizon, self.cost, model_f, \n",
        "                                              mean_obs,gamma = 1)\n",
        "        \"\"\"\n",
        "        self.policy_loss = policy_train_loss\n",
        "        total_reward = []\n",
        "        for i in range(20):\n",
        "            init_x = torch.tensor(self.env.reset()).unsqueeze(0).float() \n",
        "\n",
        "            imagine_reward = self.model.validate_by_imagination(init_x, self.policy.forward, plan, mean_obs = False)\n",
        "            total_reward.append(imagine_reward)\n",
        "            #print('temp reward', imagine_reward)\n",
        "        mean_reward = np.mean(total_reward)\n",
        "        std_reward = np.std(total_reward)\n",
        "        print('Training reward: mean {}, std {}.'.format(mean_reward, std_reward))\n",
        "        \n",
        "        return mean_reward, std_reward\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        total_cost10 = []\n",
        "        total_cost100 = []\n",
        "        for i in range(20):\n",
        "            initial_state = torch.tensor(self.env.reset()).unsqueeze(0).float()         #[1, output]\n",
        "            mean_cost10 = self.policy.rp_validate(num_particle, initial_state, 10, self.cost, model_f, mean_obs)\n",
        "            mean_cost100 = self.policy.rp_validate(num_particle, initial_state, 100, self.cost, model_f, mean_obs)\n",
        "            total_cost10.append(mean_cost10)\n",
        "            total_cost100.append(mean_cost100)\n",
        "\n",
        "        mean_cost10 = np.mean(total_cost10)\n",
        "        std_cost10 = np.std(total_cost10)\n",
        "        mean_cost100 = np.mean(total_cost100)\n",
        "        std_cost100 = np.std(total_cost100)\n",
        "\n",
        "        return mean_cost10, std_cost10, mean_cost100, std_cost100\n",
        "        \"\"\"\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpSBZWAc-YDQ"
      },
      "source": [
        "#Learning-Planning iterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9_4M1m1-Z-d",
        "outputId": "02c185b3-03f9-4ae5-f2df-b43c205a40a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "time1 = time.time()\n",
        "\n",
        "testing_reward_list = []\n",
        "behaviour_uncertainty = False\n",
        "deterministic = False\n",
        "plan = 'rp'\n",
        "num_data = 10\n",
        "mean_training_reward_list = []\n",
        "std_training_reward_list = []\n",
        "\n",
        "torch.manual_seed(1)\n",
        "agent = Agent(env_case = 1, device = 'cuda', mode = 'LSTM')\n",
        "for i in range(num_data):\n",
        "     _ = agent.env_rollout(True, behaviour_uncertainty = behaviour_uncertainty,plan = 'random')\n",
        "\n",
        "agent.model_learning(num_epoch=1000, num_batch = 10)\n",
        "#agent.model = rssm\n",
        "#mean_training_reward, std_training_reward = \n",
        "agent.policy_learning(imagine_num=50, num_particle = 1000, num_epoch = 1, batch_size = 10, horizon = 10, plan = plan, mean_obs = False)\n",
        "print('\\n Finish policy learning...')\n",
        "\n",
        "#mean_training_reward_list.append(mean_training_reward)\n",
        "#std_training_reward_list.append(std_training_reward)\n",
        "\n",
        "\n",
        "print('\\n ------------------TESTING-------------------')\n",
        "#over 10 trails\n",
        "avg_rewards = 0\n",
        "for j in range(20):\n",
        "    rewards = agent.env_rollout(if_remember=False, behaviour_uncertainty = behaviour_uncertainty,plan = plan)\n",
        "    print(j, rewards)\n",
        "    avg_rewards += rewards\n",
        "avg_rewards = avg_rewards/20\n",
        "testing_reward_list.append(avg_rewards)\n",
        "print('Total trajs:', j, avg_rewards)\n",
        "if avg_rewards > 200:\n",
        "    print('success')\n",
        "        \n",
        "\n",
        "avg_data_length_list = []\n",
        "mean_cost10_list = []\n",
        "std_cost10_list = []\n",
        "mean_cost100_list = []\n",
        "std_cost100_list = []\n",
        "\n",
        "test_std_list = []\n",
        "\n",
        "\n",
        "for i in range(50):\n",
        "    print('Epoch = ',i+1)\n",
        "    _ = agent.env_rollout(True, behaviour_uncertainty = behaviour_uncertainty,plan = plan)\n",
        "\n",
        "    total = 0\n",
        "    for i in range(len(agent.observations_list)):\n",
        "        total += len(agent.observations_list[i])\n",
        "    print('average training data length = ', total/len(agent.observations_list))\n",
        "    avg_data_length_list.append(total/len(agent.observations_list))\n",
        "\n",
        "    agent.model_learning(num_epoch=1000, num_batch = 10)\n",
        "\n",
        "    #mean_training_reward, std_training_reward=\n",
        "    #agent.policy = controller(1, 4, 'cuda').to('cuda')\n",
        "    agent.policy_learning(imagine_num=50, num_particle = 1000, num_epoch = 1, batch_size = 10, horizon = 10, plan = plan, mean_obs = False)\n",
        "    #mean_training_reward_list.append(mean_training_reward)\n",
        "    #std_training_reward_list.append(std_training_reward)\n",
        "\n",
        "    print('\\n Finish policy learning...')\n",
        "    \"\"\"\n",
        "    total_cost10 = []\n",
        "    for i in range(20):\n",
        "        initial_state = torch.tensor(agent.env.reset()).unsqueeze(0).float()\n",
        "        mean_cost = agent.policy.rp_validate(num_particle=1000, initial_state = initial_state, \n",
        "                                             horizon = 10 ,cost_f = agent.cost, model_imagine_f=\n",
        "                                             agent.model.imagine, mean_obs=False)\n",
        "        total_cost10.append(mean_cost)\n",
        "    mean_cost10 = np.mean(total_cost10)\n",
        "    std_cost10 = np.mean(total_cost10)\n",
        "    mean_cost10_list.append(mean_cost10)\n",
        "    std_cost10_list.append(std_cost10)\n",
        "\n",
        "    total_cost100 = []\n",
        "    for i in range(20):\n",
        "        initial_state = torch.tensor(agent.env.reset()).unsqueeze(0).float()\n",
        "        mean_cost = agent.policy.rp_validate(num_particle=1000, initial_state = initial_state, \n",
        "                                             horizon = 100 ,cost_f = agent.cost, model_imagine_f=\n",
        "                                             agent.model.imagine, mean_obs = False)        \n",
        "        total_cost100.append(mean_cost)\n",
        "    mean_cost100 = np.mean(total_cost100)\n",
        "    std_cost100 = np.mean(total_cost100)\n",
        "    mean_cost100_list.append(mean_cost100)\n",
        "    std_cost100_list.append(std_cost100)\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    if i%5==0:\n",
        "        temp_std = 0\n",
        "        for j in range(10):\n",
        "\n",
        "            testing_obs = testing_obs_list[j].unsqueeze(1).float()\n",
        "            testing_actions = testing_actions_list[j].float()\n",
        "            init_obs = testing_obs[0,:,:]\n",
        "\n",
        "            mean, std = agent.model.mc_predict(init_obs, testing_actions, mean_obs = False)          #[seq-1,1, output]\n",
        "            temp_std += std.mean(0).detach().cpu()\n",
        "        test_std_list.append(temp_std/10)\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    print('\\n ------------------TESTING-------------------')\n",
        "    #over 10 trails\n",
        "    avg_rewards = 0\n",
        "    for j in range(20):\n",
        "        rewards = agent.env_rollout(if_remember=False, behaviour_uncertainty = behaviour_uncertainty,plan = plan)\n",
        "        print(j, rewards)\n",
        "        avg_rewards += rewards\n",
        "    avg_rewards = avg_rewards/20\n",
        "    testing_reward_list.append(avg_rewards)\n",
        "    print('Total trajs:', j, avg_rewards)\n",
        "    if avg_rewards > 200:\n",
        "        print('success')\n",
        "\n",
        "time2 = time.time()\n",
        "print(time2 - time1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CartPoleModEnv - Version 0.2.0, Noise case: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch0; FE = -5.973501205444336; LL = -4.559955596923828; KL = 1.4135457277297974.\n",
            "\n",
            " Finish policy learning...\n",
            "\n",
            " ------------------TESTING-------------------\n",
            "0 18\n",
            "1 20\n",
            "2 24\n",
            "3 23\n",
            "4 17\n",
            "5 19\n",
            "6 20\n",
            "7 17\n",
            "8 23\n",
            "9 18\n",
            "10 25\n",
            "11 21\n",
            "12 24\n",
            "13 17\n",
            "14 22\n",
            "15 17\n",
            "16 22\n",
            "17 25\n",
            "18 26\n",
            "19 18\n",
            "Total trajs: 19 20.8\n",
            "Epoch =  1\n",
            "average training data length =  13.363636363636363\n",
            "Epoch0; FE = 6.617893695831299; LL = 6.893275260925293; KL = 0.2753816545009613.\n",
            "\n",
            " Finish policy learning...\n",
            "\n",
            " ------------------TESTING-------------------\n",
            "0 21\n",
            "1 32\n",
            "2 25\n",
            "3 23\n",
            "4 29\n",
            "5 32\n",
            "6 26\n",
            "7 23\n",
            "8 20\n",
            "9 24\n",
            "10 28\n",
            "11 31\n",
            "12 33\n",
            "13 26\n",
            "14 28\n",
            "15 28\n",
            "16 34\n",
            "17 23\n",
            "18 25\n",
            "19 30\n",
            "Total trajs: 19 27.05\n",
            "Epoch =  2\n",
            "average training data length =  14.333333333333334\n",
            "Epoch0; FE = 8.081825256347656; LL = 8.250612258911133; KL = 0.16878710687160492.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-5af0a9f68179>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mavg_data_length_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m#mean_training_reward, std_training_reward=\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-719a94b95dcc>\u001b[0m in \u001b[0;36mmodel_learning\u001b[0;34m(self, num_epoch, num_batch)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mb_FE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_LL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_KL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrun_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrun_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mb_FE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBJ-YIJt-Lva"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}